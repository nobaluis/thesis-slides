<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Learning Dexterous Manipulation with Model-Free Deep Reinforcement Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="slides_files/reveal.js-3.3.0.1/css/reveal.css"/>



<link rel="stylesheet" href="slides_files/reveal.js-3.3.0.1/css/theme/white.css" id="theme">


  <!-- some tweaks to reveal css -->
  <style type="text/css">
    .reveal h1 { font-size: 2.0em; }
    .reveal h2 { font-size: 1.5em;  }
    .reveal h3 { font-size: 1.25em;	}
    .reveal h4 { font-size: 1em;	}

    .reveal .slides>section,
    .reveal .slides>section>section {
      padding: 0px 0px;
    }



    .reveal table {
      border-width: 1px;
      border-spacing: 2px;
      border-style: dotted;
      border-color: gray;
      border-collapse: collapse;
      font-size: 0.7em;
    }

    .reveal table th {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      font-weight: bold;
      border-style: dotted;
      border-color: gray;
    }

    .reveal table td {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      border-style: dotted;
      border-color: gray;
    }


  </style>

    <style type="text/css">code{white-space: pre;}</style>

    <link rel="stylesheet" href="style.css"/>

<!-- Printing and PDF exports -->
<script id="paper-css" type="application/dynamic-css">

/* Default Print Stylesheet Template
   by Rob Glazebrook of CSSnewbie.com
   Last Updated: June 4, 2008

   Feel free (nay, compelled) to edit, append, and
   manipulate this file as you see fit. */


@media print {

	/* SECTION 1: Set default width, margin, float, and
	   background. This prevents elements from extending
	   beyond the edge of the printed page, and prevents
	   unnecessary background images from printing */
	html {
		background: #fff;
		width: auto;
		height: auto;
		overflow: visible;
	}
	body {
		background: #fff;
		font-size: 20pt;
		width: auto;
		height: auto;
		border: 0;
		margin: 0 5%;
		padding: 0;
		overflow: visible;
		float: none !important;
	}

	/* SECTION 2: Remove any elements not needed in print.
	   This would include navigation, ads, sidebars, etc. */
	.nestedarrow,
	.controls,
	.fork-reveal,
	.share-reveal,
	.state-background,
	.reveal .progress,
	.reveal .backgrounds {
		display: none !important;
	}

	/* SECTION 3: Set body font face, size, and color.
	   Consider using a serif font for readability. */
	body, p, td, li, div {
		font-size: 20pt!important;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		color: #000;
	}

	/* SECTION 4: Set heading font face, sizes, and color.
	   Differentiate your headings from your body text.
	   Perhaps use a large sans-serif for distinction. */
	h1,h2,h3,h4,h5,h6 {
		color: #000!important;
		height: auto;
		line-height: normal;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		text-shadow: 0 0 0 #000 !important;
		text-align: left;
		letter-spacing: normal;
	}
	/* Need to reduce the size of the fonts for printing */
	h1 { font-size: 28pt !important;  }
	h2 { font-size: 24pt !important; }
	h3 { font-size: 22pt !important; }
	h4 { font-size: 22pt !important; font-variant: small-caps; }
	h5 { font-size: 21pt !important; }
	h6 { font-size: 20pt !important; font-style: italic; }

	/* SECTION 5: Make hyperlinks more usable.
	   Ensure links are underlined, and consider appending
	   the URL to the end of the link for usability. */
	a:link,
	a:visited {
		color: #000 !important;
		font-weight: bold;
		text-decoration: underline;
	}
	/*
	.reveal a:link:after,
	.reveal a:visited:after {
		content: " (" attr(href) ") ";
		color: #222 !important;
		font-size: 90%;
	}
	*/


	/* SECTION 6: more reveal.js specific additions by @skypanther */
	ul, ol, div, p {
		visibility: visible;
		position: static;
		width: auto;
		height: auto;
		display: block;
		overflow: visible;
		margin: 0;
		text-align: left !important;
	}
	.reveal pre,
	.reveal table {
		margin-left: 0;
		margin-right: 0;
	}
	.reveal pre code {
		padding: 20px;
		border: 1px solid #ddd;
	}
	.reveal blockquote {
		margin: 20px 0;
	}
	.reveal .slides {
		position: static !important;
		width: auto !important;
		height: auto !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 0 !important;
		zoom: 1 !important;

		overflow: visible !important;
		display: block !important;

		text-align: left !important;
		-webkit-perspective: none;
		   -moz-perspective: none;
		    -ms-perspective: none;
		        perspective: none;

		-webkit-perspective-origin: 50% 50%;
		   -moz-perspective-origin: 50% 50%;
		    -ms-perspective-origin: 50% 50%;
		        perspective-origin: 50% 50%;
	}
	.reveal .slides section {
		visibility: visible !important;
		position: static !important;
		width: auto !important;
		height: auto !important;
		display: block !important;
		overflow: visible !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 60px 20px !important;
		z-index: auto !important;

		opacity: 1 !important;

		page-break-after: always !important;

		-webkit-transform-style: flat !important;
		   -moz-transform-style: flat !important;
		    -ms-transform-style: flat !important;
		        transform-style: flat !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;

		-webkit-transition: none !important;
		   -moz-transition: none !important;
		    -ms-transition: none !important;
		        transition: none !important;
	}
	.reveal .slides section.stack {
		padding: 0 !important;
	}
	.reveal section:last-of-type {
		page-break-after: avoid !important;
	}
	.reveal section .fragment {
		opacity: 1 !important;
		visibility: visible !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;
	}
	.reveal section img {
		display: block;
		margin: 15px 0px;
		background: rgba(255,255,255,1);
		border: 1px solid #666;
		box-shadow: none;
	}

	.reveal section small {
		font-size: 0.8em;
	}

}  
</script>


<script id="pdf-css" type="application/dynamic-css">
    
/**
 * This stylesheet is used to print reveal.js
 * presentations to PDF.
 *
 * https://github.com/hakimel/reveal.js#pdf-export
 */

* {
	-webkit-print-color-adjust: exact;
}

body {
	margin: 0 auto !important;
	border: 0;
	padding: 0;
	float: none !important;
	overflow: visible;
}

html {
	width: 100%;
	height: 100%;
	overflow: visible;
}

/* Remove any elements not needed in print. */
.nestedarrow,
.reveal .controls,
.reveal .progress,
.reveal .playback,
.reveal.overview,
.fork-reveal,
.share-reveal,
.state-background {
	display: none !important;
}

h1, h2, h3, h4, h5, h6 {
	text-shadow: 0 0 0 #000 !important;
}

.reveal pre code {
	overflow: hidden !important;
	font-family: Courier, 'Courier New', monospace !important;
}

ul, ol, div, p {
	visibility: visible;
	position: static;
	width: auto;
	height: auto;
	display: block;
	overflow: visible;
	margin: auto;
}
.reveal {
	width: auto !important;
	height: auto !important;
	overflow: hidden !important;
}
.reveal .slides {
	position: static;
	width: 100%;
	height: auto;

	left: auto;
	top: auto;
	margin: 0 !important;
	padding: 0 !important;

	overflow: visible;
	display: block;

	-webkit-perspective: none;
	   -moz-perspective: none;
	    -ms-perspective: none;
	        perspective: none;

	-webkit-perspective-origin: 50% 50%; /* there isn't a none/auto value but 50-50 is the default */
	   -moz-perspective-origin: 50% 50%;
	    -ms-perspective-origin: 50% 50%;
	        perspective-origin: 50% 50%;
}

.reveal .slides section {
	page-break-after: always !important;

	visibility: visible !important;
	position: relative !important;
	display: block !important;
	position: relative !important;

	margin: 0 !important;
	padding: 0 !important;
	box-sizing: border-box !important;
	min-height: 1px;

	opacity: 1 !important;

	-webkit-transform-style: flat !important;
	   -moz-transform-style: flat !important;
	    -ms-transform-style: flat !important;
	        transform-style: flat !important;

	-webkit-transform: none !important;
	   -moz-transform: none !important;
	    -ms-transform: none !important;
	        transform: none !important;
}

.reveal section.stack {
	margin: 0 !important;
	padding: 0 !important;
	page-break-after: avoid !important;
	height: auto !important;
	min-height: auto !important;
}

.reveal img {
	box-shadow: none;
}

.reveal .roll {
	overflow: visible;
	line-height: 1em;
}

/* Slide backgrounds are placed inside of their slide when exporting to PDF */
.reveal section .slide-background {
	display: block !important;
	position: absolute;
	top: 0;
	left: 0;
	width: 100%;
	z-index: -1;
}

/* All elements should be above the slide-background */
.reveal section>* {
	position: relative;
	z-index: 1;
}

/* Display slide speaker notes when 'showNotes' is enabled */
.reveal .speaker-notes-pdf {
	display: block;
	width: 100%;
	max-height: none;
	left: auto;
	top: auto;
	z-index: 100;
}

/* Display slide numbers when 'slideNumber' is enabled */
.reveal .slide-number-pdf {
	display: block;
	position: absolute;
	font-size: 14px;
}

</script>


<script>
var style = document.createElement( 'style' );
style.type = 'text/css';
var style_script_id = window.location.search.match( /print-pdf/gi ) ? 'pdf-css' : 'paper-css';
var style_script = document.getElementById(style_script_id).text;
style.innerHTML = style_script;
document.getElementsByTagName('head')[0].appendChild(style);
</script>

    <script src="slides_files/htmlwidgets-1.5.1/htmlwidgets.js"></script>
    <script src="slides_files/plotly-binding-4.9.2.1/plotly.js"></script>
    <script src="slides_files/typedarray-0.1/typedarray.min.js"></script>
    <script src="slides_files/jquery-1.11.3/jquery.min.js"></script>
    <link href="slides_files/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
    <script src="slides_files/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
    <link href="slides_files/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
    <script src="slides_files/plotly-main-1.52.2/plotly-latest.min.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">Learning Dexterous Manipulation with Model-Free Deep Reinforcement Learning</h1>
    <h2 class="author"><div class="line-block"><small>Thesis Advisor: Ph.D. Reyes Ríos Cabreara</small><br />
<small>Author: Luis Castillo</small></div></h2>
</section>

<section id="presentation-outline" class="slide level2">
<h2>Presentation outline</h2>
<ul>
<li><a href="#/introduction">Introduction</a></li>
<li><a href="#/state-of-the-art">State-of-the-art</a></li>
<li><a href="#/proposal-description">Proposal Description</a>
<ul>
<li><a href="#/motivation">Motivation</a></li>
<li><a href="#/background">Background</a></li>
<li><a href="#/problem-statement">Problem Statement</a></li>
<li><a href="#/hypothesis">Hypothesis</a></li>
<li><a href="#/objectives">Objectives</a></li>
<li><a href="#/scientific-contribution">Scientific Contribution</a></li>
</ul></li>
<li><a href="#/activities">Chronogram of activities</a></li>
<li><a href="#/references">References</a></li>
</ul>
</section>
<section><section id="introduction" class="title-slide slide level1"><h1>Introduction</h1></section><section id="section" class="slide level2">
<h2></h2>
<p><strong>Deep Reinforcement Learning</strong> (Deep RL) methods have great potential in real-world application. They have recently shown that it can solve tasks with superior human performance in some scenarios, e.g. classic Atari games <a href="#/ref">(Mnih et al. 2013)</a>, massive online multiplayer games <a href="#/ref">(OpenAI 2018)</a>, or complicated board games <a href="#/ref">(Silver et al. 2017)</a>.</p>
<video data-autoplay src="videos/alphazero_2017_final.m4v" loop="loop" width="460" height="258">
</video>
<video data-autoplay src="videos/openai_five_dota_final.m4v" loop="loop" width="460" height="258">
</video>
</section><section id="section-1" class="slide level2">
<h2></h2>
<p>Deep RL algorithms can be used to learn to control physical robots in real-time <a href="#/ref">(Gao et al. 2020)</a>. Deep RL agents learn to solve a given task by interacting with an unknown, unstructured environment. Due to its simple and intuitive fundamentals and its great potential to solve complex problems, Deep RL has become one of the most appealing branches of artificial intelligence.</p>
<video data-autoplay src="videos/dxl-tracker-trpo.m4v" loop="loop" width="280" height="157">
</video>
<video data-autoplay src="videos/create-docker-trpo.m4v" loop="loop" width="280" height="157">
</video>
<video data-autoplay src="videos/ur-reacher-6-trpo.m4v" loop="loop" width="280" height="157">
</video>
</section></section>
<section><section id="state-of-the-art" class="title-slide slide level1"><h1>State of the art</h1></section><section id="section-2" class="slide level2">
<h2></h2>
<ul>
<li><p><a href="#/ref">(Peng et al. 2018)</a> propose to use dynamics randomization to train recurrent policies in simulation, and deploy the learned policies directly on a physical robot, achieving good performance on an object pushing task, without calibration.</p></li>
<li><p><a href="#/ref">(OpenAI et al. 2018)</a> proposes to learn dexterous manipulation with a five finger articulated hand, using Proximal Policy Optimization (PPO), with dynamic randomization (DR) in the simulation, achieving very good results in the physical system.</p></li>
<li><p><a href="#/ref1">(Akkaya et al. 2019)</a> proposes a new algorithm to modify the dynamics of the simulator automatically called Automatic Domain Randomization (ADR), substantially improving the results obtained with DR. This is demonstrated by solving the Rubik’s cube with a robotic hand.</p></li>
</ul>
<!-- TODO: add the tossing bot paper -->
</section></section>
<section><section id="proposal-description" class="title-slide slide level1"><h1>Proposal description</h1></section><section id="motivation" class="slide level2">
<h2>Motivation</h2>
<p>In most cases, the dynamic process of <strong>robotic control</strong> can be approximated as a <strong>Markov Decision Process</strong> (MDP) making it an ideal field to experiment with Deep RL algorithms.</p>
<p>In addition, recently, large technology companies and prestigious research centers have focused their research in this area, such as OpenAI, who solved the Rubik’s cube with a five-finger articulated robotic hand completely in simulation.</p>
<video data-autoplay src="videos/shadow_rubik_normal.m4v" loop="loop" width="460" height="258">
</video>
<video data-autoplay src="videos/shadow_rubik_oclution.m4v" loop="loop" width="460" height="258">
</video>
</section><section id="section-3" class="slide level2">
<h2></h2>
<p>Training in simulation and then transferring learned policy to physical systems (i.e <strong>sim-to-real</strong>) or using expert human demonstrations are two approaches that satisfy computational and safety requirements in robot learning tasks. Furthermore, robot simulators have been widely developed for decades, e.g. CoppeliaSim, MuJoCo, Gazebo and Pybullet.</p>
<video data-autoplay src="videos/shadow_hand_block.mp4" loop="loop" width="292" height="292">
</video>
<video data-autoplay src="videos/shadow_hand_real_block_final.m4v" loop="loop" width="520" height="292">
</video>
</section><section id="background" class="slide level2">
<h2>Background</h2>
<p>Machine learning (ML) is the field of AI that studies the process of learning from data to make predictions and/or decisions. It is generally categorized into supervised, unsupervised and reinforcement learning.</p>
<p><img data-src="images/mrl_classification.png" alt="Machine Learning classification" style="width:50.0%" /></p>
</section><section id="reinforcement-learning" class="slide level2">
<h2>Reinforcement learning</h2>
<p>Reinforcement Learning (RL) is a subfield of ML that address the problem of automatic learning for optimal decisions over time. The main characters of the RL are the agent and the environment. The environment is the world where the agent lives and with which he interacts.</p>
<p><img data-src="images/agent_env_loop2.png" alt="The agent, environment loop" style="width:60.0%" /></p>
</section><section id="deep-reinforcement-learning" class="slide level2">
<h2>Deep Reinforcement Learning</h2>
<p>DRL is to combine the advantages of DL and RL for building AI systems. The main reason to use DL in RL is to leverage the scalability of DNN in high-dimensional space, e.g. the value function approximation utilizes the data representation of DNN to represent the highly compositional data distribution through end-to-end gradient-based optimization.</p>
<center>
<video data-autoplay src="videos/rl_atari_final.m4v" loop="loop" width="520" height="292">
</video>
</center>
</section><section id="problem-statement" class="slide level2">
<h2>Problem Statement</h2>
<p>Humans can solve many activities that are presented to us in our daily life without much effort, e.g. move and throw objects, open doors or write. However, although these tasks may seem simple, they require a certain degree of dexterous, which we humans learn through experience.</p>
<p><img data-src="images/dexterous_manipulation.png" style="width:80.0%" /></p>
</section><section id="section-4" class="slide level2">
<h2></h2>
<p>Trying to solve <em>these activities</em> with a robot following traditional robot control approaches represents a big challenge, because it is necessary to solve complex dynamic models and consider many uncertainties in the process.</p>
<p><img data-src="images/model_based_task.png" /></p>
</section><section id="section-5" class="slide level2">
<h2></h2>
<p>However, the latest advances in RL applied to robotic control have shown that it is possible to teach robots to solve this kind of tasks similarly to how a baby learn to walk; by trial and error.</p>
<p><img data-src="images/drl_diagram.png" /></p>
</section><section id="section-6" class="slide level2">
<h2></h2>
<p>This learning can be obtained directly on the physical robot, but this involves a couple of problems:</p>
<ol type="1">
<li><p>Data sampling is very <strong>inefficient</strong> and it would take hundreds of hours to solve a task, in addition to the need to design an automatic system to restart the physical test environment or directly depend on human operators.</p></li>
<li><p>And some Deep RL methods employ scanning mechanisms that could result in <strong>dangerous</strong> actions for the robot and the surrounding environment.</p></li>
</ol>
</section><section id="section-7" class="slide level2">
<h2></h2>
<p>A very attractive alternative is gather all the data necessary for the training purely in simulation, and then deploy the learned control policy in a physical robot.</p>
<video data-autoplay src="videos/fetch_push.mp4" loop="loop" width="300" height="300">
</video>
<video data-autoplay src="videos/shadow_hand_block.mp4" loop="loop" width="300" height="300">
</video>
<video data-autoplay src="videos/humanoid.mp4" loop="loop" width="300" height="300">
</video>
</section><section id="section-8" class="slide level2">
<h2></h2>
<p>However, simulation environments do not represent the full complexity of the real world, and the policies learned in these virtual environments only perform well under conditions similar to those seen during the training phase. This disparity between virtual simulation environments and the real world is known as the <em>“reality gap”</em>.</p>
</section><section id="section-9" class="slide level2">
<h2></h2>
<p>But, in the latest advances in Deep RL applied to robot control have shown <a href="#/ref">(Peng et al. 2018sim, Akkaya et al. 2019)</a> that it is possible to overcome this barrier using techniques that randomly modify the dynamics of the simulator during the training phase in order to expose the agent to a wide range of variations in the environment, this forces the agent to <strong>learn to adapt</strong> to the constant changes in the environment.</p>
<p><img data-src="images/dr_crop.png" style="width:80.0%" /></p>
</section><section id="hypothesis" class="slide level2">
<h2>Hypothesis</h2>
<blockquote>
<p>If we randomly modify the dynamics of a simulation environment in order to learn a robust control policy for solving a dexterous manipulation task, then it is possible to transfer this learned policy to a physical robot and solve the task without any fine-tuning.</p>
</blockquote>
</section><section id="objetives" class="slide level2">
<h2>General objective</h2>
<p>The general objective of this thesis project is:</p>
<p>Train an agent purely in simulation to solve a specific dexterous manipulation task using reinforcement learning algorithms and domain randomization techniques to transfer the learned policy in a physical robot</p>
</section><section id="specific-objectives" class="slide level2">
<h2>Specific objectives</h2>
<p>The following objectives emerge from the main objective of this research:</p>
<ol type="1">
<li>Select a state-of-the-art simulator that meets the requirements of the selected task and reinforcement learning algorithms.</li>
<li>Document me in the use of the selected robotics simulator and generate the virtual test environment for the selected task.</li>
<li>Review of the state-of-the-art in reinforcement learning algorithms for robot control.</li>
</ol>
</section><section id="section-10" class="slide level2">
<h2></h2>
<ol start="4" type="1">
<li>Implement one or more of the state-of-the-art RL algorithms for robot control to solve the selected task in the simulation environment (without domain randomization).</li>
<li>Review of the state-of-the-art of domain randomization techniques.</li>
<li>Implement state-of-the-art techniques in domain randomization and retrain the agent to solve the selected task.</li>
<li>Transfer the policies learned in simulation (with and without domain randomization) to the physical robot and make a comparison of performance in the selected task.</li>
<li>Communicate the results.</li>
</ol>
</section><section id="scientific-contribution" class="slide level2">
<h2>Scientific contribution</h2>
<p>The main contribution of this work is to solve a dexterous manipulation task<span class="math inline">\(^{*}\)</span> with a redundant serial robot by training an agent purely in simulation.</p>
</section></section>
<section><section id="activities" class="title-slide slide level1"><h1>Chronogram of activities</h1></section><section id="section-11" class="slide level2">
<h2></h2>
<div id="htmlwidget-114b04edf04fdeef168f" style="width:768px;height:576px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-114b04edf04fdeef168f">{"x":{"visdat":{"330957ce6f0e5":["function () ","plotlyVisDat"]},"cur_data":"330957ce6f0e5","attrs":{"330957ce6f0e5":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2020-07-15","2020-09-13"],"y":[10,10],"mode":"lines","line":{"color":"#E78AC3","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Review Deep RL literature <br> Duration:  60 days<br> Type:  Research","evaluate":true,"inherit":true},"330957ce6f0e5.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2020-09-15","2020-09-30"],"y":[9,9],"mode":"lines","line":{"color":"#E78AC3","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Select state-of-the-art simulator <br> Duration:  15 days<br> Type:  Research","evaluate":true,"inherit":true},"330957ce6f0e5.2":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2020-09-30","2020-11-29"],"y":[8,8],"mode":"lines","line":{"color":"#8DA0CB","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Learn to use the simulator <br> Duration:  60 days<br> Type:  Learning","evaluate":true,"inherit":true},"330957ce6f0e5.3":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2020-12-01","2020-12-31"],"y":[7,7],"mode":"lines","line":{"color":"#FC8D62","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Create simulation environment <br> Duration:  30 days<br> Type:  Develop","evaluate":true,"inherit":true},"330957ce6f0e5.4":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2021-01-01","2021-01-31"],"y":[6,6],"mode":"lines","line":{"color":"#FC8D62","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Train first agent without DR/ADR <br> Duration:  30 days<br> Type:  Develop","evaluate":true,"inherit":true},"330957ce6f0e5.5":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2021-02-01","2021-02-16"],"y":[5,5],"mode":"lines","line":{"color":"#E78AC3","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Review state-of-the-art DR/ADR <br> Duration:  15 days<br> Type:  Research","evaluate":true,"inherit":true},"330957ce6f0e5.6":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2021-02-15","2021-03-17"],"y":[4,4],"mode":"lines","line":{"color":"#FC8D62","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Tranin second agent with DR/ADR <br> Duration:  30 days<br> Type:  Develop","evaluate":true,"inherit":true},"330957ce6f0e5.7":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2021-03-15","2021-04-14"],"y":[3,3],"mode":"lines","line":{"color":"#FC8D62","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Transfer policies to real robot <br> Duration:  30 days<br> Type:  Develop","evaluate":true,"inherit":true},"330957ce6f0e5.8":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2021-04-15","2021-05-15"],"y":[2,2],"mode":"lines","line":{"color":"#66C2A5","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Make a comparative between both policies <br> Duration:  30 days<br> Type:  Analysis","evaluate":true,"inherit":true},"330957ce6f0e5.9":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2021-03-15","2021-07-13"],"y":[1,1],"mode":"lines","line":{"color":"#A6D854","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Write the thesis <br> Duration:  120 days<br> Type:  Writing","evaluate":true,"inherit":true}},"layout":{"width":900,"height":600,"margin":{"b":40,"l":60,"t":25,"r":10},"xaxis":{"domain":[0,1],"automargin":true,"showgrid":true,"tickfont":{"color":"#000000"},"title":[]},"yaxis":{"domain":[0,0.9],"automargin":true,"showgrid":true,"tickfont":{"color":"#000000"},"tickmode":"array","tickvals":[10,9,8,7,6,5,4,3,2,1],"ticktext":["Review Deep RL literature","Select state-of-the-art simulator","Learn to use the simulator","Create simulation environment","Train first agent without DR/ADR","Review state-of-the-art DR/ADR","Tranin second agent with DR/ADR","Transfer policies to real robot","Make a comparative between both policies","Write the thesis"],"title":[]},"legend":{"orientation":"h"},"hovermode":"closest","showlegend":false},"source":"A","config":{"showSendToCloud":false},"data":[{"x":["2020-07-15","2020-09-13"],"y":[10,10],"mode":"lines","line":{"color":"#E78AC3","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Review Deep RL literature <br> Duration:  60 days<br> Type:  Research","Task:  Review Deep RL literature <br> Duration:  60 days<br> Type:  Research"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2020-09-15","2020-09-30"],"y":[9,9],"mode":"lines","line":{"color":"#E78AC3","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Select state-of-the-art simulator <br> Duration:  15 days<br> Type:  Research","Task:  Select state-of-the-art simulator <br> Duration:  15 days<br> Type:  Research"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(255,127,14,1)","line":{"color":"rgba(255,127,14,1)"}},"error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2020-09-30","2020-11-29"],"y":[8,8],"mode":"lines","line":{"color":"#8DA0CB","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Learn to use the simulator <br> Duration:  60 days<br> Type:  Learning","Task:  Learn to use the simulator <br> Duration:  60 days<br> Type:  Learning"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(44,160,44,1)","line":{"color":"rgba(44,160,44,1)"}},"error_y":{"color":"rgba(44,160,44,1)"},"error_x":{"color":"rgba(44,160,44,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2020-12-01","2020-12-31"],"y":[7,7],"mode":"lines","line":{"color":"#FC8D62","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Create simulation environment <br> Duration:  30 days<br> Type:  Develop","Task:  Create simulation environment <br> Duration:  30 days<br> Type:  Develop"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(214,39,40,1)","line":{"color":"rgba(214,39,40,1)"}},"error_y":{"color":"rgba(214,39,40,1)"},"error_x":{"color":"rgba(214,39,40,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2021-01-01","2021-01-31"],"y":[6,6],"mode":"lines","line":{"color":"#FC8D62","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Train first agent without DR/ADR <br> Duration:  30 days<br> Type:  Develop","Task:  Train first agent without DR/ADR <br> Duration:  30 days<br> Type:  Develop"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(148,103,189,1)","line":{"color":"rgba(148,103,189,1)"}},"error_y":{"color":"rgba(148,103,189,1)"},"error_x":{"color":"rgba(148,103,189,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2021-02-01","2021-02-16"],"y":[5,5],"mode":"lines","line":{"color":"#E78AC3","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Review state-of-the-art DR/ADR <br> Duration:  15 days<br> Type:  Research","Task:  Review state-of-the-art DR/ADR <br> Duration:  15 days<br> Type:  Research"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(140,86,75,1)","line":{"color":"rgba(140,86,75,1)"}},"error_y":{"color":"rgba(140,86,75,1)"},"error_x":{"color":"rgba(140,86,75,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2021-02-15","2021-03-17"],"y":[4,4],"mode":"lines","line":{"color":"#FC8D62","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Tranin second agent with DR/ADR <br> Duration:  30 days<br> Type:  Develop","Task:  Tranin second agent with DR/ADR <br> Duration:  30 days<br> Type:  Develop"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(227,119,194,1)","line":{"color":"rgba(227,119,194,1)"}},"error_y":{"color":"rgba(227,119,194,1)"},"error_x":{"color":"rgba(227,119,194,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2021-03-15","2021-04-14"],"y":[3,3],"mode":"lines","line":{"color":"#FC8D62","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Transfer policies to real robot <br> Duration:  30 days<br> Type:  Develop","Task:  Transfer policies to real robot <br> Duration:  30 days<br> Type:  Develop"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(127,127,127,1)","line":{"color":"rgba(127,127,127,1)"}},"error_y":{"color":"rgba(127,127,127,1)"},"error_x":{"color":"rgba(127,127,127,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2021-04-15","2021-05-15"],"y":[2,2],"mode":"lines","line":{"color":"#66C2A5","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Make a comparative between both policies <br> Duration:  30 days<br> Type:  Analysis","Task:  Make a comparative between both policies <br> Duration:  30 days<br> Type:  Analysis"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(188,189,34,1)","line":{"color":"rgba(188,189,34,1)"}},"error_y":{"color":"rgba(188,189,34,1)"},"error_x":{"color":"rgba(188,189,34,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2021-03-15","2021-07-13"],"y":[1,1],"mode":"lines","line":{"color":"#A6D854","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Write the thesis <br> Duration:  120 days<br> Type:  Writing","Task:  Write the thesis <br> Duration:  120 days<br> Type:  Writing"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(23,190,207,1)","line":{"color":"rgba(23,190,207,1)"}},"error_y":{"color":"rgba(23,190,207,1)"},"error_x":{"color":"rgba(23,190,207,1)"},"xaxis":"x","yaxis":"y","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</section></section>
<section><section id="references" class="title-slide slide level1"><h1>References</h1></section><section id="section-12" class="slide level2">
<h2></h2>
<ol type="1">
<li><p>Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik’s cube with a robot hand. arXiv preprint arXiv:1910.07113, 2019.</p></li>
<li><p>OpenAI, Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pa- chocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas Schneider, Szymon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba. Learning dexterous in-hand manipulation, 2018.</p></li>
<li><p>Wenbo Gao, Laura Graesser, Krzysztof Choromanski, Xingyou Song, Nevena Lazic, Pannag Sanketi, Vikas Sindhwani, and Navdeep Jaitly. Robotic table tennis with model-free reinforcement learning. arXiv preprint arXiv:2003.14398, 2020.</p></li>
</ol>
</section><section id="section-13" class="slide level2">
<h2></h2>
<ol start="4" type="1">
<li><p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.</p></li>
<li><p>OpenAI. Openai five. <a href="https://blog.openai.com/openai-five/" class="uri">https://blog.openai.com/openai-five/</a>, 2018.</p></li>
<li><p>Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In 2018 IEEE international conference on robotics and automation (ICRA), pages 1–8. IEEE, 2018.</p></li>
<li><p>David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017.</p></li>
</ol>
</section><section id="section-14" class="slide level2">
<h2></h2>
<ol start="8" type="1">
<li><p>Xingyou Song, Yuxiang Yang, Krzysztof Choromanski, Ken Caluwaerts, Wenbo Gao, Chelsea Finn, and Jie Tan. Rapidly adaptable legged robots via evolutionary meta-learning. arXiv preprint arXiv:2003.01239, 2020.</p></li>
<li><p>Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint arXiv:1804.10332, 2018.</p></li>
<li><p>Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 23–30. IEEE, 2017.</p></li>
<li><p>Andy Zeng, Shuran Song, Johnny Lee, Alberto Rodriguez, and Thomas Funkhouser. Tossingbot: Learning to throw arbitrary objects with residual physics. IEEE Transactions on Robotics, 2020.</p></li>
</ol>
</section></section>
    </div>
  </div>

  <script src="slides_files/reveal.js-3.3.0.1/lib/js/head.min.js"></script>
  <script src="slides_files/reveal.js-3.3.0.1/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display the page number of the current slide
        slideNumber: true,
        // Push each slide change to the browser history
        history: true,
        // Vertical centering of slides
        center: true,
        // Opens links in an iframe preview overlay
        previewLinks: false,
        // Transition style
        transition: 'none', // none/fade/slide/convex/concave/zoom
        // Transition style for full page slide backgrounds
        backgroundTransition: 'default', // none/fade/slide/convex/concave/zoom



        // Optional reveal.js plugins
        dependencies: [
        ]
      });
    </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

<script>
  (function() {
    if (window.jQuery) {
      Reveal.addEventListener( 'slidechanged', function(event) {  
        window.jQuery(event.previousSlide).trigger('hidden');
        window.jQuery(event.currentSlide).trigger('shown');
      });
    }
  })();
</script>


  </body>
</html>
