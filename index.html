<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Learning Dexterous Manipulation with Model-Free Deep Reinforcement Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="slides_files/reveal.js-3.3.0.1/css/reveal.css"/>



<link rel="stylesheet" href="slides_files/reveal.js-3.3.0.1/css/theme/white.css" id="theme">


  <!-- some tweaks to reveal css -->
  <style type="text/css">
    .reveal h1 { font-size: 2.0em; }
    .reveal h2 { font-size: 1.5em;  }
    .reveal h3 { font-size: 1.25em;	}
    .reveal h4 { font-size: 1em;	}

    .reveal .slides>section,
    .reveal .slides>section>section {
      padding: 0px 0px;
    }



    .reveal table {
      border-width: 1px;
      border-spacing: 2px;
      border-style: dotted;
      border-color: gray;
      border-collapse: collapse;
      font-size: 0.7em;
    }

    .reveal table th {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      font-weight: bold;
      border-style: dotted;
      border-color: gray;
    }

    .reveal table td {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      border-style: dotted;
      border-color: gray;
    }


  </style>

    <style type="text/css">code{white-space: pre;}</style>

    <link rel="stylesheet" href="style.css"/>

<!-- Printing and PDF exports -->
<script id="paper-css" type="application/dynamic-css">

/* Default Print Stylesheet Template
   by Rob Glazebrook of CSSnewbie.com
   Last Updated: June 4, 2008

   Feel free (nay, compelled) to edit, append, and
   manipulate this file as you see fit. */


@media print {

	/* SECTION 1: Set default width, margin, float, and
	   background. This prevents elements from extending
	   beyond the edge of the printed page, and prevents
	   unnecessary background images from printing */
	html {
		background: #fff;
		width: auto;
		height: auto;
		overflow: visible;
	}
	body {
		background: #fff;
		font-size: 20pt;
		width: auto;
		height: auto;
		border: 0;
		margin: 0 5%;
		padding: 0;
		overflow: visible;
		float: none !important;
	}

	/* SECTION 2: Remove any elements not needed in print.
	   This would include navigation, ads, sidebars, etc. */
	.nestedarrow,
	.controls,
	.fork-reveal,
	.share-reveal,
	.state-background,
	.reveal .progress,
	.reveal .backgrounds {
		display: none !important;
	}

	/* SECTION 3: Set body font face, size, and color.
	   Consider using a serif font for readability. */
	body, p, td, li, div {
		font-size: 20pt!important;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		color: #000;
	}

	/* SECTION 4: Set heading font face, sizes, and color.
	   Differentiate your headings from your body text.
	   Perhaps use a large sans-serif for distinction. */
	h1,h2,h3,h4,h5,h6 {
		color: #000!important;
		height: auto;
		line-height: normal;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		text-shadow: 0 0 0 #000 !important;
		text-align: left;
		letter-spacing: normal;
	}
	/* Need to reduce the size of the fonts for printing */
	h1 { font-size: 28pt !important;  }
	h2 { font-size: 24pt !important; }
	h3 { font-size: 22pt !important; }
	h4 { font-size: 22pt !important; font-variant: small-caps; }
	h5 { font-size: 21pt !important; }
	h6 { font-size: 20pt !important; font-style: italic; }

	/* SECTION 5: Make hyperlinks more usable.
	   Ensure links are underlined, and consider appending
	   the URL to the end of the link for usability. */
	a:link,
	a:visited {
		color: #000 !important;
		font-weight: bold;
		text-decoration: underline;
	}
	/*
	.reveal a:link:after,
	.reveal a:visited:after {
		content: " (" attr(href) ") ";
		color: #222 !important;
		font-size: 90%;
	}
	*/


	/* SECTION 6: more reveal.js specific additions by @skypanther */
	ul, ol, div, p {
		visibility: visible;
		position: static;
		width: auto;
		height: auto;
		display: block;
		overflow: visible;
		margin: 0;
		text-align: left !important;
	}
	.reveal pre,
	.reveal table {
		margin-left: 0;
		margin-right: 0;
	}
	.reveal pre code {
		padding: 20px;
		border: 1px solid #ddd;
	}
	.reveal blockquote {
		margin: 20px 0;
	}
	.reveal .slides {
		position: static !important;
		width: auto !important;
		height: auto !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 0 !important;
		zoom: 1 !important;

		overflow: visible !important;
		display: block !important;

		text-align: left !important;
		-webkit-perspective: none;
		   -moz-perspective: none;
		    -ms-perspective: none;
		        perspective: none;

		-webkit-perspective-origin: 50% 50%;
		   -moz-perspective-origin: 50% 50%;
		    -ms-perspective-origin: 50% 50%;
		        perspective-origin: 50% 50%;
	}
	.reveal .slides section {
		visibility: visible !important;
		position: static !important;
		width: auto !important;
		height: auto !important;
		display: block !important;
		overflow: visible !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 60px 20px !important;
		z-index: auto !important;

		opacity: 1 !important;

		page-break-after: always !important;

		-webkit-transform-style: flat !important;
		   -moz-transform-style: flat !important;
		    -ms-transform-style: flat !important;
		        transform-style: flat !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;

		-webkit-transition: none !important;
		   -moz-transition: none !important;
		    -ms-transition: none !important;
		        transition: none !important;
	}
	.reveal .slides section.stack {
		padding: 0 !important;
	}
	.reveal section:last-of-type {
		page-break-after: avoid !important;
	}
	.reveal section .fragment {
		opacity: 1 !important;
		visibility: visible !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;
	}
	.reveal section img {
		display: block;
		margin: 15px 0px;
		background: rgba(255,255,255,1);
		border: 1px solid #666;
		box-shadow: none;
	}

	.reveal section small {
		font-size: 0.8em;
	}

}  
</script>


<script id="pdf-css" type="application/dynamic-css">
    
/**
 * This stylesheet is used to print reveal.js
 * presentations to PDF.
 *
 * https://github.com/hakimel/reveal.js#pdf-export
 */

* {
	-webkit-print-color-adjust: exact;
}

body {
	margin: 0 auto !important;
	border: 0;
	padding: 0;
	float: none !important;
	overflow: visible;
}

html {
	width: 100%;
	height: 100%;
	overflow: visible;
}

/* Remove any elements not needed in print. */
.nestedarrow,
.reveal .controls,
.reveal .progress,
.reveal .playback,
.reveal.overview,
.fork-reveal,
.share-reveal,
.state-background {
	display: none !important;
}

h1, h2, h3, h4, h5, h6 {
	text-shadow: 0 0 0 #000 !important;
}

.reveal pre code {
	overflow: hidden !important;
	font-family: Courier, 'Courier New', monospace !important;
}

ul, ol, div, p {
	visibility: visible;
	position: static;
	width: auto;
	height: auto;
	display: block;
	overflow: visible;
	margin: auto;
}
.reveal {
	width: auto !important;
	height: auto !important;
	overflow: hidden !important;
}
.reveal .slides {
	position: static;
	width: 100%;
	height: auto;

	left: auto;
	top: auto;
	margin: 0 !important;
	padding: 0 !important;

	overflow: visible;
	display: block;

	-webkit-perspective: none;
	   -moz-perspective: none;
	    -ms-perspective: none;
	        perspective: none;

	-webkit-perspective-origin: 50% 50%; /* there isn't a none/auto value but 50-50 is the default */
	   -moz-perspective-origin: 50% 50%;
	    -ms-perspective-origin: 50% 50%;
	        perspective-origin: 50% 50%;
}

.reveal .slides section {
	page-break-after: always !important;

	visibility: visible !important;
	position: relative !important;
	display: block !important;
	position: relative !important;

	margin: 0 !important;
	padding: 0 !important;
	box-sizing: border-box !important;
	min-height: 1px;

	opacity: 1 !important;

	-webkit-transform-style: flat !important;
	   -moz-transform-style: flat !important;
	    -ms-transform-style: flat !important;
	        transform-style: flat !important;

	-webkit-transform: none !important;
	   -moz-transform: none !important;
	    -ms-transform: none !important;
	        transform: none !important;
}

.reveal section.stack {
	margin: 0 !important;
	padding: 0 !important;
	page-break-after: avoid !important;
	height: auto !important;
	min-height: auto !important;
}

.reveal img {
	box-shadow: none;
}

.reveal .roll {
	overflow: visible;
	line-height: 1em;
}

/* Slide backgrounds are placed inside of their slide when exporting to PDF */
.reveal section .slide-background {
	display: block !important;
	position: absolute;
	top: 0;
	left: 0;
	width: 100%;
	z-index: -1;
}

/* All elements should be above the slide-background */
.reveal section>* {
	position: relative;
	z-index: 1;
}

/* Display slide speaker notes when 'showNotes' is enabled */
.reveal .speaker-notes-pdf {
	display: block;
	width: 100%;
	max-height: none;
	left: auto;
	top: auto;
	z-index: 100;
}

/* Display slide numbers when 'slideNumber' is enabled */
.reveal .slide-number-pdf {
	display: block;
	position: absolute;
	font-size: 14px;
}

</script>


<script>
var style = document.createElement( 'style' );
style.type = 'text/css';
var style_script_id = window.location.search.match( /print-pdf/gi ) ? 'pdf-css' : 'paper-css';
var style_script = document.getElementById(style_script_id).text;
style.innerHTML = style_script;
document.getElementsByTagName('head')[0].appendChild(style);
</script>

    <script src="slides_files/header-attrs-2.6.6/header-attrs.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">Learning Dexterous Manipulation with Model-Free Deep Reinforcement Learning</h1>
  <h1 class="subtitle">Thesis Advance III</h1>
    <h2 class="author"><div class="line-block"><small>Thesis Director: Ph.D. Reyes Ríos Cabrera</small><br />
<small>Author: Luis Castillo</small></div></h2>
</section>

<section id="presentation-outline" class="slide level2">
<h2>Presentation outline</h2>
<ul>
<li><a href="#/simulators">Learning in simulation</a></li>
<li><a href="#/isaac">Isaac Sim</a></li>
<li><a href="#/project_review">Project review</a></li>
<li><a href="#/case_study">Case of study</a></li>
<li><a href="#/task">Task to solve</a></li>
<li><a href="#/references">References</a></li>
</ul>
</section>
<section>
<section id="simulators" class="title-slide slide level1">
<h1>Learning in simulation</h1>

</section>
<section id="current-approach" class="slide level2">
<h2>Current approach</h2>
<table>
<colgroup>
<col style="width: 62%" />
<col style="width: 10%" />
<col style="width: 4%" />
<col style="width: 9%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="header">
<th>Publication</th>
<th style="text-align: center;">Author</th>
<th style="text-align: center;">Year</th>
<th style="text-align: center;">Simulator</th>
<th style="text-align: center;">Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Learning 6-DOF Grasping Interaction via Deep Geometry-aware 3D Representations</td>
<td style="text-align: center;">Yan et al.</td>
<td style="text-align: center;">2018</td>
<td style="text-align: center;">Pybullet</td>
<td style="text-align: center;">grasping</td>
</tr>
<tr class="even">
<td>Sim-to-Real: Learning Agile Locomotion For Quadruped Robots</td>
<td style="text-align: center;">Tan et al.</td>
<td style="text-align: center;">2018</td>
<td style="text-align: center;">Pybullet</td>
<td style="text-align: center;">locomotion</td>
</tr>
<tr class="odd">
<td>A Sim2real method based on DDQN for training a self-driving scale car</td>
<td style="text-align: center;">Zhang et al.</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">Unity</td>
<td style="text-align: center;">self-driving car</td>
</tr>
<tr class="even">
<td>Multi-Agent Manipulation via Locomotion using Hierarchical Sim2Real</td>
<td style="text-align: center;">Nachum et al.</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">Mujoco</td>
<td style="text-align: center;">locomotion</td>
</tr>
<tr class="odd">
<td>DeepRacer: Educational Autonomous Racing Platform for Experimentation with Sim2Real Reinforcement Learning</td>
<td style="text-align: center;">Balaji et al.</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">Gazebo</td>
<td style="text-align: center;">self-driving car</td>
</tr>
<tr class="even">
<td>Solving Rubik’s Cube with a Robot Hand</td>
<td style="text-align: center;">Akkaya et al.</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">Mujoco + Unity</td>
<td style="text-align: center;">in-hand manipulation</td>
</tr>
<tr class="odd">
<td>The Importance and the Limitations of Sim2Real for Robotic Manipulation in Precision Agriculture</td>
<td style="text-align: center;">Rizzardo et al.</td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Gazebo</td>
<td style="text-align: center;">manipulation</td>
</tr>
<tr class="even">
<td>Sim2Real for Self-Supervised Monocular Depth and Segmentation</td>
<td style="text-align: center;">Raghavan et al.</td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Unity</td>
<td style="text-align: center;">segmentation</td>
</tr>
<tr class="odd">
<td>TossingBot: Learning to Throw Arbitrary Objects with Residual Physics</td>
<td style="text-align: center;">Zeng et al.</td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Pybullet</td>
<td style="text-align: center;">throwing objects</td>
</tr>
</tbody>
</table>
</section>
<section id="sota-simulators" class="slide level2">
<h2>SOTA simulators</h2>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 12%" />
<col style="width: 20%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="header">
<th>Simulator</th>
<th style="text-align: center;">Physics</th>
<th style="text-align: center;">Photorealistic</th>
<th style="text-align: center;">DR</th>
<th style="text-align: center;">ROS</th>
<th style="text-align: center;">GPU</th>
<th style="text-align: center;">Multi-GPU</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Gazebo</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">❌</td>
<td style="text-align: center;">❌</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">❌</td>
</tr>
<tr class="even">
<td>Pybullet</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">❌</td>
<td style="text-align: center;">❌</td>
<td style="text-align: center;">❌</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">❌</td>
</tr>
<tr class="odd">
<td>MuJoCo</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">❌</td>
<td style="text-align: center;">❌</td>
<td style="text-align: center;">❌</td>
<td style="text-align: center;">❌</td>
<td style="text-align: center;">❌</td>
</tr>
<tr class="even">
<td>Unity</td>
<td style="text-align: center;">❌</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">❌</td>
<td style="text-align: center;">❌</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">❌</td>
</tr>
<tr class="odd">
<td>Isaac Sim</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">✅</td>
</tr>
</tbody>
</table>
</section>
<section id="simulator-gap" class="slide level2">
<h2>Simulator Gap</h2>
<p><img data-src="images/simulation_gap.png" style="width:80.0%" /></p>
</section></section>
<section>
<section id="isaac" class="title-slide slide level1">
<h1>Isaac Sim</h1>

</section>
<section id="nvidia-isaac-sim" class="slide level2">
<h2>NVIDIA Isaac Sim</h2>
<p>The Isaac Sim is powered by the NVIDIA Omniverse, a platform for real-time simulation and 3D production pipelines with support for multi-GPU.</p>
<table>
<thead>
<tr>
<th>
Specs
</th>
<th>
Example
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<div>
<ul>
<li>
Navigation and manipulation tasks
</li>
<li>
Support for Isaac SDK and ROS
</li>
<li>
RGB-D, Lidar and IMU sensor
</li>
<li>
Domain Randomization
</li>
<li>
Ground truth labeling
</li>
<li>
Segmentation and BBs
</li>
</ul>
</div>
</td>
<td>
<div>
<video data-autoplay src="videos/isaac_main_fill_bin.m4v" loop="loop" width="440" height="247">
</video>
</div>
</td>
</tr>
</tbody>
</table>
</section>
<section id="main-features-13-simulate-reality" class="slide level2">
<h2>Main features (1/3): Simulate Reality</h2>
<p>Omniverse physics currently includes Rigid Body Dynamics, Destruction and Fracture, Vehicle Dynamics and Fluid Dynamics (Flow)</p>
<center>
<video data-autoplay src="videos/physix_demo.m4v" loop="loop" width="720" height="405">
</video>
</center>
</section>
<section id="main-features-23-rtx-renderer" class="slide level2">
<h2>Main features (2/3): RTX Renderer</h2>
<p>Omniverse supports RTX Renderer that fully exploits hardware RT (Turing &amp; Ampere) to do real-time hardware-accelerated <strong>ray-tracing</strong> and <strong>path-tracing</strong></p>
<center>
<video data-autoplay src="videos/rtx_demo.m4v" loop="loop" width="720" height="405">
</video>
</center>
</section>
<section id="main-features-33-domain-randomization" class="slide level2">
<h2>Main features (3/3): Domain Randomization</h2>
<p>DR varies the parameters that define a scene in the simulation env. Some parameters include pose, scale, the lighting in simulation, the appearance of objects (color and textures)</p>
<!-- One of the main objectives of domain randomization is to enhance the training of deep learning applications by exposing the neural network to a wide variety of domain parameters in simulation which will help to generalize well to real world applications.  -->
<center>
<video data-autoplay src="videos/dr_demo.m4v" loop="loop" width="720" height="405">
</video>
</center>
</section>
<section id="how-it-works" class="slide level2">
<h2>How it works</h2>
<p><img data-src="images/omni_pipeline.png" alt="Omniverse pipeline" /></p>
</section></section>
<section>
<section id="project_review" class="title-slide slide level1">
<h1>Project review</h1>

</section>
<section id="hypothesis" class="slide level2">
<h2>Hypothesis</h2>
<blockquote>
<p>If we train a Deep RL agent in a state-of-the-art simulator where the simulation parameters are varied, then it’s possible to learn a robust policy to solve a specific dexterous manipulation task.</p>
</blockquote>
</section>
<section id="general-objective" class="slide level2">
<h2>General objective</h2>
<p>The general objective of this thesis project is:</p>
<ul>
<li>Train a Deep RL Agent in simulation to solve a specific manipulation task using a modern model-free RL algorithm and domain randomization techniques.</li>
</ul>
</section>
<section id="specific-objectives" class="slide level2">
<h2>Specific objectives</h2>
<p>In order to achieve the main objective, the following specific objectives are considered:</p>
<ol type="1">
<li><p>Train a DDN-based model with the PPO <span class="citation" data-cites="schulman2017">(Schulman et al. 2017)</span> algorithm in a <strong>virtual environment</strong> with domain randomization, using a full-state observation.</p></li>
<li><p>Same as above, but this time using a partial observation of the environment (RGB images).</p></li>
<li><p>Evaluate different strategies and determine the best methods.</p></li>
</ol>
</section></section>
<section>
<section id="case_study" class="title-slide slide level1">
<h1>Case of study: Nvidia Jetracer</h1>
<p>In order to be proficient in the use of the Omniverse platform and the training of Deep RL models in simulation. I follow the documentation to recreate one of the RL training samples.</p>
<p><img data-src="images/isaac_jetracer.png" style="width:70.0%" alt="Nvidia Jetracer env" /></p>
</section>
<section id="task-description" class="slide level2">
<h2>Task description</h2>
<p>The Nvidia JetRacer is an autonomous AI race-car powered by a Jetson Nano. The idea is to train an agent in simulation to follow the center lane using the camera image as observation.</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Environment</strong></td>
<td><strong>Observation</strong></td>
</tr>
<tr class="even">
<td><img data-src="images/isaac_jetracer_path.png" style="width:50.0%" /></td>
<td><img data-src="images/jetracer_camera_sample.png" /></td>
</tr>
</tbody>
</table>
<ul>
<li><span class="math inline">\(A \in \mathbb{R}^2\)</span>: motors velocity (steering and forward).</li>
<li><span class="math inline">\(O \in \mathbb{R}^{224 \times 224 \times 3}\)</span>: RGB images from front camera.</li>
</ul>
</section>
<section id="reward-function" class="slide level2">
<h2>Reward function</h2>
<p>The reward returned for the environment is:</p>
<p><span class="math display">\[
R = \dot{x} e^{\frac{-x^{2}}{\sigma^{2}}}
\]</span></p>
<p>Where <span class="math inline">\(\dot{x}\)</span> is the current velocity of the Jetracer, <span class="math inline">\(x\)</span> is the shortest distance from the robot to the center line of the track, and <span class="math inline">\(\sigma\)</span> is a hyper parameter.</p>
</section>
<section id="model" class="slide level2">
<h2>Model</h2>
<!-- The input of the model is an image of $224 \times 224$ RGB image scaled to $[0,1]$. -->
<!-- These images are generated by the camera in simulation after $n$ steps. -->
<!-- During each step, the physics engine process the physical contacts and the RTX component render the images. -->
<p><img data-src="images/jetracer_model_arch.png" /></p>
</section>
<section id="training" class="slide level2">
<h2>Training</h2>
<p><img data-src="images/omniverse_training_loop.png" /></p>
</section>
<section id="results-12" class="slide level2">
<h2>Results (1/2)</h2>
<p><img src="slides_files/figure-revealjs/res-1.png" width="768" /></p>
</section>
<section id="results-22" class="slide level2">
<h2>Results (2/2)</h2>
<p>After around 200k steps, we see the agent starting to follow the center lane. Even with the changes of light and the random distractors.</p>
<table>
<tbody>
<tr>
<td>
<div>
<video data-autoplay src="videos/jetracer/jetracer_results-125-149_encoded.m4v" loop="loop" width="440" height="362">
</div>
</td>
<td>
<div>
<video data-autoplay src="videos/jetracer/jetracer_results-74-100_encoded.m4v" loop="loop" width="440" height="362">
</video>
</div>
</td>
</tr>
</tbody>
</table>
</section></section>
<section>
<section id="task" class="title-slide slide level1">
<h1>Task to solve</h1>

</section>
<section id="task_des" class="slide level2">
<h2>Task description</h2>
<p>The Franka Panda is a serial robot with 7 DOF. The idea is to train an agent to stack the blocks in a tower, without throwing any block.</p>
<table>
<tbody>
<tr class="odd">
<td><img data-src="images/isaac_stack_task_01.png" /></td>
<td><img data-src="images/isaac_stack_task_04.png" /></td>
</tr>
<tr class="even">
<td><img data-src="images/isaac_stack_task_03.png" /></td>
<td><img data-src="images/isaac_stack_task_02.png" /></td>
</tr>
</tbody>
</table>
</section>
<section id="task-details" class="slide level2">
<h2>Task details</h2>
<p><img data-src="images/task_description.png" /></p>
</section>
<section id="domain-randomization-in-the-task" class="slide level2">
<h2>Domain randomization in the task</h2>
<table>
<thead>
<tr class="header">
<th>Position</th>
<th>Scale</th>
<th>Color</th>
<th>Light</th>
<th>Texture</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img data-src="images/dr/isaac_dr_4_mov_comp_sample.gif" /></td>
<td><img data-src="images/dr/isaac_dr_8_scale_comp_sample.gif" /></td>
<td><img data-src="images/dr/isaac_dr_2_color_comp_sample.gif" /></td>
<td><img data-src="images/dr/isaac_dr_10_light_comp_sample.gif" /></td>
<td><img data-src="images/dr/isaac_dr_12_texture_comp_sample.gif" /></td>
</tr>
</tbody>
</table>
</section>
<section id="proposal_sol" class="slide level2">
<h2>Proposal solution</h2>
<ol type="1">
<li><p>Train the agent using a <strong>full-satate observation</strong>, using all available information (position, velocity) as input for a less complex network architecture, but using the same training method (PPO).</p></li>
<li><p>Train the agent using <strong>partial-state observation</strong>, that is the image of one or two cameras in the environment, using a similar architecture to the Jetracer example and PPO.</p></li>
</ol>
<p><img data-src="images/proposal_sols.svg" /></p>
</section>
<section id="current_status" class="slide level2">
<h2>Current status</h2>
<p><img data-src="images/project_status_03.png" /></p>
</section></section>
<section>
<section id="references" class="title-slide slide level1">
<h1>References</h1>

</section>
<section id="section" class="slide level2">
<h2></h2>
<div id="refs" class="references csl-bib-body hanging-indent" style="width: 1000px; height: 600px; overflow-y: scroll;" role="doc-bibliography">
<div id="ref-balaji2019" class="csl-entry" role="doc-biblioentry">
Balaji, Bharathan, Sunil Mallya, Sahika Genc, Saurabh Gupta, Leo Dirac, Vineet Khare, Gourav Roy, et al. 2019. <span>“<span>DeepRacer</span>: <span>Educational Autonomous Racing Platform</span> for <span>Experimentation</span> with <span>Sim2Real Reinforcement Learning</span>.”</span> <em>arXiv:1911.01562 [Cs]</em>, November. <a href="http://arxiv.org/abs/1911.01562">http://arxiv.org/abs/1911.01562</a>.
</div>
<div id="ref-brockman2016" class="csl-entry" role="doc-biblioentry">
Brockman, Greg, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. 2016. <span>“<span>OpenAI Gym</span>.”</span> <em>arXiv:1606.01540 [Cs]</em>, June. <a href="http://arxiv.org/abs/1606.01540">http://arxiv.org/abs/1606.01540</a>.
</div>
<div id="ref-gomes2021" class="csl-entry" role="doc-biblioentry">
Gomes, Daniel Fernandes, Paolo Paoletti, and Shan Luo. 2021. <span>“Generation of <span>GelSight Tactile Images</span> for <span>Sim2Real Learning</span>.”</span> <em>arXiv:2101.07169 [Cs]</em>, January. <a href="http://arxiv.org/abs/2101.07169">http://arxiv.org/abs/2101.07169</a>.
</div>
<div id="ref-kaspar2020" class="csl-entry" role="doc-biblioentry">
Kaspar, Manuel, Juan David Munoz Osorio, and Jürgen Bock. 2020. <span>“<span>Sim2Real Transfer</span> for <span>Reinforcement Learning</span> Without <span>Dynamics Randomization</span>.”</span> <em>arXiv:2002.11635 [Cs]</em>, February. <a href="http://arxiv.org/abs/2002.11635">http://arxiv.org/abs/2002.11635</a>.
</div>
<div id="ref-mnih2016" class="csl-entry" role="doc-biblioentry">
Mnih, Volodymyr, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. <span>“Asynchronous <span>Methods</span> for <span>Deep Reinforcement Learning</span>.”</span> <em>arXiv:1602.01783 [Cs]</em>, June. <a href="http://arxiv.org/abs/1602.01783">http://arxiv.org/abs/1602.01783</a>.
</div>
<div id="ref-nachum2019" class="csl-entry" role="doc-biblioentry">
Nachum, Ofir, Michael Ahn, Hugo Ponte, Shixiang Gu, and Vikash Kumar. 2019. <span>“Multi-<span>Agent Manipulation</span> via <span>Locomotion</span> Using <span>Hierarchical Sim2Real</span>.”</span> <em>arXiv:1908.05224 [Cs]</em>, October. <a href="http://arxiv.org/abs/1908.05224">http://arxiv.org/abs/1908.05224</a>.
</div>
<div id="ref-openai2019" class="csl-entry" role="doc-biblioentry">
OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, et al. 2019. <span>“Solving <span>Rubik</span>’s <span>Cube</span> with a <span>Robot Hand</span>.”</span> <em>arXiv:1910.07113 [Cs, Stat]</em>, October. <a href="http://arxiv.org/abs/1910.07113">http://arxiv.org/abs/1910.07113</a>.
</div>
<div id="ref-stable-baselines3" class="csl-entry" role="doc-biblioentry">
Raffin, Antonin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah Dormann. 2019. <span>“Stable Baselines3.”</span> <em>GitHub Repository</em>. <a href="https://github.com/DLR-RM/stable-baselines3">https://github.com/DLR-RM/stable-baselines3</a>; GitHub.
</div>
<div id="ref-raghavan2020" class="csl-entry" role="doc-biblioentry">
Raghavan, Nithin, Punarjay Chakravarty, and Shubham Shrivastava. 2020. <span>“<span>Sim2Real</span> for <span>Self</span>-<span>Supervised Monocular Depth</span> and <span>Segmentation</span>.”</span> <em>arXiv:2012.00238 [Cs]</em>, November. <a href="http://arxiv.org/abs/2012.00238">http://arxiv.org/abs/2012.00238</a>.
</div>
<div id="ref-rizzardo2020" class="csl-entry" role="doc-biblioentry">
Rizzardo, Carlo, Sunny Katyara, Miguel Fernandes, and Fei Chen. 2020. <span>“The <span>Importance</span> and the <span>Limitations</span> of <span>Sim2Real</span> for <span>Robotic Manipulation</span> in <span>Precision Agriculture</span>.”</span> <em>arXiv:2008.03983 [Cs]</em>, August. <a href="http://arxiv.org/abs/2008.03983">http://arxiv.org/abs/2008.03983</a>.
</div>
<div id="ref-schulman2017" class="csl-entry" role="doc-biblioentry">
Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. <span>“Proximal <span>Policy Optimization Algorithms</span>.”</span> <em>arXiv:1707.06347 [Cs]</em>, August. <a href="http://arxiv.org/abs/1707.06347">http://arxiv.org/abs/1707.06347</a>.
</div>
<div id="ref-tan2018" class="csl-entry" role="doc-biblioentry">
Tan, Jie, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke. 2018. <span>“Sim-to-<span>Real</span>: <span>Learning Agile Locomotion For Quadruped Robots</span>.”</span> <em>arXiv:1804.10332 [Cs]</em>, May. <a href="http://arxiv.org/abs/1804.10332">http://arxiv.org/abs/1804.10332</a>.
</div>
<div id="ref-yan2018" class="csl-entry" role="doc-biblioentry">
Yan, Xinchen, Jasmine Hsu, Mohi Khansari, Yunfei Bai, Arkanath Pathak, Abhinav Gupta, James Davidson, and Honglak Lee. 2018. <span>“Learning 6-<span>DOF Grasping Interaction</span> via <span>Deep Geometry</span>-Aware <span>3d Representations</span>.”</span> <em>arXiv:1708.07303 [Cs]</em>, June. <a href="http://arxiv.org/abs/1708.07303">http://arxiv.org/abs/1708.07303</a>.
</div>
<div id="ref-zeng2020" class="csl-entry" role="doc-biblioentry">
Zeng, Andy, Shuran Song, Johnny Lee, Alberto Rodriguez, and Thomas Funkhouser. 2020. <span>“<span>TossingBot</span>: <span>Learning</span> to <span>Throw Arbitrary Objects</span> with <span>Residual Physics</span>.”</span> <em>arXiv:1903.11239 [Cs, Stat]</em>, May. <a href="http://arxiv.org/abs/1903.11239">http://arxiv.org/abs/1903.11239</a>.
</div>
<div id="ref-zhang2019" class="csl-entry" role="doc-biblioentry">
Zhang, Qi, Tao Du, and Changzheng Tian. 2019. <span>“A <span>Sim2real</span> Method Based on <span>DDQN</span> for Training a Self-Driving Scale Car.”</span> <em>Mathematical Foundations of Computing</em> 2 (4): 315. <a href="https://doi.org/10.3934/mfc.2019020">https://doi.org/10.3934/mfc.2019020</a>.
</div>
</div>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<br>
<center>
<h1>
Thank you for your attention ☕
</h1>
</center>
</section></section>
    </div>
  </div>

  <script src="slides_files/reveal.js-3.3.0.1/lib/js/head.min.js"></script>
  <script src="slides_files/reveal.js-3.3.0.1/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display the page number of the current slide
        slideNumber: true,
        // Push each slide change to the browser history
        history: true,
        // Vertical centering of slides
        center: true,
        // Opens links in an iframe preview overlay
        previewLinks: false,
        // Transition style
        transition: 'none', // none/fade/slide/convex/concave/zoom
        // Transition style for full page slide backgrounds
        backgroundTransition: 'default', // none/fade/slide/convex/concave/zoom



        // Optional reveal.js plugins
        dependencies: [
        ]
      });
    </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

<script>
  (function() {
    if (window.jQuery) {
      Reveal.addEventListener( 'slidechanged', function(event) {  
        window.jQuery(event.previousSlide).trigger('hidden');
        window.jQuery(event.currentSlide).trigger('shown');
      });
    }
  })();
</script>


  </body>
</html>
