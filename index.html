<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Learning Dexterous Manipulation with Model-Free Deep Reinforcement Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="slides_files/reveal.js-3.3.0.1/css/reveal.css"/>



<link rel="stylesheet" href="slides_files/reveal.js-3.3.0.1/css/theme/white.css" id="theme">


  <!-- some tweaks to reveal css -->
  <style type="text/css">
    .reveal h1 { font-size: 2.0em; }
    .reveal h2 { font-size: 1.5em;  }
    .reveal h3 { font-size: 1.25em;	}
    .reveal h4 { font-size: 1em;	}

    .reveal .slides>section,
    .reveal .slides>section>section {
      padding: 0px 0px;
    }



    .reveal table {
      border-width: 1px;
      border-spacing: 2px;
      border-style: dotted;
      border-color: gray;
      border-collapse: collapse;
      font-size: 0.7em;
    }

    .reveal table th {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      font-weight: bold;
      border-style: dotted;
      border-color: gray;
    }

    .reveal table td {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      border-style: dotted;
      border-color: gray;
    }


  </style>

    <style type="text/css">code{white-space: pre;}</style>

    <link rel="stylesheet" href="style.css"/>

<!-- Printing and PDF exports -->
<script id="paper-css" type="application/dynamic-css">

/* Default Print Stylesheet Template
   by Rob Glazebrook of CSSnewbie.com
   Last Updated: June 4, 2008

   Feel free (nay, compelled) to edit, append, and
   manipulate this file as you see fit. */


@media print {

	/* SECTION 1: Set default width, margin, float, and
	   background. This prevents elements from extending
	   beyond the edge of the printed page, and prevents
	   unnecessary background images from printing */
	html {
		background: #fff;
		width: auto;
		height: auto;
		overflow: visible;
	}
	body {
		background: #fff;
		font-size: 20pt;
		width: auto;
		height: auto;
		border: 0;
		margin: 0 5%;
		padding: 0;
		overflow: visible;
		float: none !important;
	}

	/* SECTION 2: Remove any elements not needed in print.
	   This would include navigation, ads, sidebars, etc. */
	.nestedarrow,
	.controls,
	.fork-reveal,
	.share-reveal,
	.state-background,
	.reveal .progress,
	.reveal .backgrounds {
		display: none !important;
	}

	/* SECTION 3: Set body font face, size, and color.
	   Consider using a serif font for readability. */
	body, p, td, li, div {
		font-size: 20pt!important;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		color: #000;
	}

	/* SECTION 4: Set heading font face, sizes, and color.
	   Differentiate your headings from your body text.
	   Perhaps use a large sans-serif for distinction. */
	h1,h2,h3,h4,h5,h6 {
		color: #000!important;
		height: auto;
		line-height: normal;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		text-shadow: 0 0 0 #000 !important;
		text-align: left;
		letter-spacing: normal;
	}
	/* Need to reduce the size of the fonts for printing */
	h1 { font-size: 28pt !important;  }
	h2 { font-size: 24pt !important; }
	h3 { font-size: 22pt !important; }
	h4 { font-size: 22pt !important; font-variant: small-caps; }
	h5 { font-size: 21pt !important; }
	h6 { font-size: 20pt !important; font-style: italic; }

	/* SECTION 5: Make hyperlinks more usable.
	   Ensure links are underlined, and consider appending
	   the URL to the end of the link for usability. */
	a:link,
	a:visited {
		color: #000 !important;
		font-weight: bold;
		text-decoration: underline;
	}
	/*
	.reveal a:link:after,
	.reveal a:visited:after {
		content: " (" attr(href) ") ";
		color: #222 !important;
		font-size: 90%;
	}
	*/


	/* SECTION 6: more reveal.js specific additions by @skypanther */
	ul, ol, div, p {
		visibility: visible;
		position: static;
		width: auto;
		height: auto;
		display: block;
		overflow: visible;
		margin: 0;
		text-align: left !important;
	}
	.reveal pre,
	.reveal table {
		margin-left: 0;
		margin-right: 0;
	}
	.reveal pre code {
		padding: 20px;
		border: 1px solid #ddd;
	}
	.reveal blockquote {
		margin: 20px 0;
	}
	.reveal .slides {
		position: static !important;
		width: auto !important;
		height: auto !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 0 !important;
		zoom: 1 !important;

		overflow: visible !important;
		display: block !important;

		text-align: left !important;
		-webkit-perspective: none;
		   -moz-perspective: none;
		    -ms-perspective: none;
		        perspective: none;

		-webkit-perspective-origin: 50% 50%;
		   -moz-perspective-origin: 50% 50%;
		    -ms-perspective-origin: 50% 50%;
		        perspective-origin: 50% 50%;
	}
	.reveal .slides section {
		visibility: visible !important;
		position: static !important;
		width: auto !important;
		height: auto !important;
		display: block !important;
		overflow: visible !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 60px 20px !important;
		z-index: auto !important;

		opacity: 1 !important;

		page-break-after: always !important;

		-webkit-transform-style: flat !important;
		   -moz-transform-style: flat !important;
		    -ms-transform-style: flat !important;
		        transform-style: flat !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;

		-webkit-transition: none !important;
		   -moz-transition: none !important;
		    -ms-transition: none !important;
		        transition: none !important;
	}
	.reveal .slides section.stack {
		padding: 0 !important;
	}
	.reveal section:last-of-type {
		page-break-after: avoid !important;
	}
	.reveal section .fragment {
		opacity: 1 !important;
		visibility: visible !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;
	}
	.reveal section img {
		display: block;
		margin: 15px 0px;
		background: rgba(255,255,255,1);
		border: 1px solid #666;
		box-shadow: none;
	}

	.reveal section small {
		font-size: 0.8em;
	}

}  
</script>


<script id="pdf-css" type="application/dynamic-css">
    
/**
 * This stylesheet is used to print reveal.js
 * presentations to PDF.
 *
 * https://github.com/hakimel/reveal.js#pdf-export
 */

* {
	-webkit-print-color-adjust: exact;
}

body {
	margin: 0 auto !important;
	border: 0;
	padding: 0;
	float: none !important;
	overflow: visible;
}

html {
	width: 100%;
	height: 100%;
	overflow: visible;
}

/* Remove any elements not needed in print. */
.nestedarrow,
.reveal .controls,
.reveal .progress,
.reveal .playback,
.reveal.overview,
.fork-reveal,
.share-reveal,
.state-background {
	display: none !important;
}

h1, h2, h3, h4, h5, h6 {
	text-shadow: 0 0 0 #000 !important;
}

.reveal pre code {
	overflow: hidden !important;
	font-family: Courier, 'Courier New', monospace !important;
}

ul, ol, div, p {
	visibility: visible;
	position: static;
	width: auto;
	height: auto;
	display: block;
	overflow: visible;
	margin: auto;
}
.reveal {
	width: auto !important;
	height: auto !important;
	overflow: hidden !important;
}
.reveal .slides {
	position: static;
	width: 100%;
	height: auto;

	left: auto;
	top: auto;
	margin: 0 !important;
	padding: 0 !important;

	overflow: visible;
	display: block;

	-webkit-perspective: none;
	   -moz-perspective: none;
	    -ms-perspective: none;
	        perspective: none;

	-webkit-perspective-origin: 50% 50%; /* there isn't a none/auto value but 50-50 is the default */
	   -moz-perspective-origin: 50% 50%;
	    -ms-perspective-origin: 50% 50%;
	        perspective-origin: 50% 50%;
}

.reveal .slides section {
	page-break-after: always !important;

	visibility: visible !important;
	position: relative !important;
	display: block !important;
	position: relative !important;

	margin: 0 !important;
	padding: 0 !important;
	box-sizing: border-box !important;
	min-height: 1px;

	opacity: 1 !important;

	-webkit-transform-style: flat !important;
	   -moz-transform-style: flat !important;
	    -ms-transform-style: flat !important;
	        transform-style: flat !important;

	-webkit-transform: none !important;
	   -moz-transform: none !important;
	    -ms-transform: none !important;
	        transform: none !important;
}

.reveal section.stack {
	margin: 0 !important;
	padding: 0 !important;
	page-break-after: avoid !important;
	height: auto !important;
	min-height: auto !important;
}

.reveal img {
	box-shadow: none;
}

.reveal .roll {
	overflow: visible;
	line-height: 1em;
}

/* Slide backgrounds are placed inside of their slide when exporting to PDF */
.reveal section .slide-background {
	display: block !important;
	position: absolute;
	top: 0;
	left: 0;
	width: 100%;
	z-index: -1;
}

/* All elements should be above the slide-background */
.reveal section>* {
	position: relative;
	z-index: 1;
}

/* Display slide speaker notes when 'showNotes' is enabled */
.reveal .speaker-notes-pdf {
	display: block;
	width: 100%;
	max-height: none;
	left: auto;
	top: auto;
	z-index: 100;
}

/* Display slide numbers when 'slideNumber' is enabled */
.reveal .slide-number-pdf {
	display: block;
	position: absolute;
	font-size: 14px;
}

</script>


<script>
var style = document.createElement( 'style' );
style.type = 'text/css';
var style_script_id = window.location.search.match( /print-pdf/gi ) ? 'pdf-css' : 'paper-css';
var style_script = document.getElementById(style_script_id).text;
style.innerHTML = style_script;
document.getElementsByTagName('head')[0].appendChild(style);
</script>

    <script src="slides_files/htmlwidgets-1.5.1/htmlwidgets.js"></script>
    <script src="slides_files/plotly-binding-4.9.2.1/plotly.js"></script>
    <script src="slides_files/typedarray-0.1/typedarray.min.js"></script>
    <script src="slides_files/jquery-1.11.3/jquery.min.js"></script>
    <link href="slides_files/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
    <script src="slides_files/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
    <link href="slides_files/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
    <script src="slides_files/plotly-main-1.52.2/plotly-latest.min.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">Learning Dexterous Manipulation with Model-Free Deep Reinforcement Learning</h1>
    <h2 class="author"><div class="line-block"><small>Thesis Director: Ph.D. Reyes Ríos Cabrera</small><br />
<small>Author: Luis Castillo</small></div></h2>
</section>

<section id="presentation-outline" class="slide level2">
<h2>Presentation outline</h2>
<ul>
<li><a href="#/introduction">Introduction</a></li>
<li><a href="#/state-of-the-art">State-of-the-art</a></li>
<li><a href="#/proposal-description">Proposal Description</a>
<ul>
<li><a href="#/motivation">Motivation</a></li>
<li><a href="#/background">Background</a></li>
<li><a href="#/problem-statement">Problem Statement</a></li>
<li><a href="#/hypothesis">Hypothesis</a></li>
<li><a href="#/objectives">Objectives</a></li>
<li><a href="#/scientific-contribution">Scientific Contribution</a></li>
</ul></li>
<li><a href="#/progress">Current Progress</a></li>
<li><a href="#/activities">Chronogram of activities</a></li>
<li><a href="#/references">References</a></li>
</ul>
</section>
<section><section id="introduction" class="title-slide slide level1"><h1>Introduction</h1></section><section id="section" class="slide level2">
<h2></h2>
<p><strong>Deep Reinforcement Learning</strong> have great potential in real-world applications. Recent advances have shown that these methods can solve tasks with superior performance than humans in some scenarios, e.g. classic Atari games <span class="citation" data-cites="mnih2013playing">Mnih et al. (2013)</span>, complicated board games <span class="citation" data-cites="silver2016mastering silver2017mastering">(Silver et al. 2016, 2017)</span> or massive online multiplayer games <span class="citation" data-cites="OpenAI_dota">OpenAI (2018)</span>.</p>
<video data-autoplay src="videos/alphazero_2017_final.m4v" loop="loop" width="460" height="258">
</video>
<video data-autoplay src="videos/openai_five_dota_final.m4v" loop="loop" width="460" height="258">
</video>
</section><section id="section-1" class="slide level2">
<h2></h2>
<p>DRL is not just for games. Due to the simplicity of the RL framework. The same algorithms for solving games can be used to control physical robots in real-time <span class="citation" data-cites="gao2020robotic">Gao et al. (2020)</span>. However, DRL methods <strong>are not intended to completely replace </strong> traditional control approaches. But, these methods can be applied in <strong>specific scenarios</strong> or in combination with traditional methods.</p>
<video data-autoplay src="videos/dxl-tracker-trpo.m4v" loop="loop" width="280" height="157">
</video>
<video data-autoplay src="videos/create-docker-trpo.m4v" loop="loop" width="280" height="157">
</video>
<video data-autoplay src="videos/ur-reacher-6-trpo.m4v" loop="loop" width="280" height="157">
</video>
</section></section>
<section><section id="state-of-the-art" class="title-slide slide level1"><h1>State of the art</h1></section><section id="rl-algorithms" class="slide level2">
<h2>RL algorithms</h2>
<p><img src="images/rl_algorithms.svg" style="display: block; margin: auto;" /></p>
</section><section id="sim-to-real-14" class="slide level2">
<h2>Sim-To-Real (1/4)</h2>
<p><span class="citation" data-cites="peng2018sim">Peng et al. (2018)</span> propose to use dynamics randomization (DR) to train in simulation, and deploy the learned policies directly on a physical robot to solve pushing object task, achieving good performance.</p>
<center>
<video data-autoplay src="videos/peng2018.m4v" loop="loop" width="720" height="405">
</video>
</center>
</section><section id="sim-to-real-24" class="slide level2">
<h2>Sim-To-Real (2/4)</h2>
<p><span class="citation" data-cites="openai2018learning">OpenAI et al. (2018)</span> propose to learn dexterity of in-hand manipulation to perform object reorientation for a five-finger robotic hand, using PPO with DR in simulation. They use the same code for playing Dota 2 <span class="citation" data-cites="OpenAI_dota">OpenAI (2018)</span>.</p>
<center>
<video data-autoplay src="videos/shadow_hand_real_block_final.m4v" loop="loop" width="720" height="405">
</video>
</center>
</section><section id="sim-to-real-34" class="slide level2">
<h2>Sim-To-Real (3/4)</h2>
<p><span class="citation" data-cites="tan2018sim">Tan et al. (2018)</span> propose to learn locomotion for quadruped robots. They learn control policies in simulation and deployed on a quadruped robot. They perform system identification for tuning the simulation parameters to improve the fidelity of the physics simulator.</p>
<center>
<video data-autoplay src="videos/tan2018.m4v" loop="loop" width="520" height="292">
</video>
</center>
</section><section id="sim-to-real-44" class="slide level2">
<h2>Sim-To-Real (4/4)</h2>
<p><span class="citation" data-cites="zeng2020tossingbot">Zeng et al. (2020)</span> propose an end-to-end AI system for picking and throwing objects. Achieving 85% of accuracy, more than double that achieved with analytical solutions (40%). They train the policy in simulation and then use an automatic system for fine-tuning on physical robot.</p>
<center>
<video data-autoplay src="videos/tossingbot.m4v" loop="loop" width="520" height="292">
</video>
</center>
</section></section>
<section><section id="proposal-description" class="title-slide slide level1"><h1>Proposal description</h1></section><section id="motivation" class="slide level2">
<h2>Motivation</h2>
<p>In most cases, the dynamic process of <strong>robotic control</strong> can be approximated as an <strong>MDP</strong> making it an ideal field to experiment with DRL algorithms.</p>
<p>In addition, recently, large technology companies and prestigious research centers have focused their research in this area, such as OpenAI, who solved the Rubik’s cube with a five-finger articulated robotic hand completely in simulation.</p>
<video data-autoplay src="videos/shadow_rubik_normal.m4v" loop="loop" width="460" height="258">
</video>
<video data-autoplay src="videos/shadow_rubik_oclution.m4v" loop="loop" width="460" height="258">
</video>
</section><section id="training-in-a-physical-world" class="slide level2">
<h2>Training in a physical world</h2>
<p>We can use DRL algorithms to train robust control policies directly in the physical robot. But this approach has some disadvantages:</p>
<ul>
<li><p><strong>Sampling inefficient</strong>: The training may take thousands of hours to solve a single task.</p></li>
<li><p><strong>Safety issues</strong>: DRL methods use exploration techniques that can be result in dangerous actions.</p></li>
</ul>
</section><section id="training-in-simulation" class="slide level2">
<h2>Training in simulation</h2>
<p>Training in simulation and then transferring learned policy to physical systems (i.e, <strong>sim-to-real</strong>) or using expert human demonstrations are two approaches that satisfy computational and safety requirements in robot learning tasks.</p>
<video data-autoplay src="videos/shadow_hand_block.mp4" loop="loop" width="292" height="292">
</video>
<video data-autoplay src="videos/shadow_hand_real_block_final.m4v" loop="loop" width="520" height="292">
</video>
</section><section id="background" class="slide level2">
<h2>Background: RL</h2>
<p>Reinforcement Learning (RL) is a subfield of ML that address the problem of automatic learning for optimal decisions over time. The main characters of the RL are the agent and the environment. The environment is the world where the agent lives and with which he interacts.</p>
<p><img src="images/agent_env_loop3.png" width="70%" style="display: block; margin: auto;" /></p>
</section><section id="deep-reinforcement-learning" class="slide level2">
<h2>Deep Reinforcement Learning</h2>
<p>DRL combine the advantages of DL and RL for building AI systems. The main reason to use DL in RL is to leverage the scalability of DNN in high-dimensional space. <span class="citation" data-cites="mnih2013playing">Mnih et al. (2013)</span> prove this by training a DNN from raw pixels to play classic arcade games.</p>
<center>
<video data-autoplay src="videos/rl_atari_final.m4v" loop="loop" width="520" height="292">
</video>
</center>
</section><section id="problem-statement" class="slide level2">
<h2>Problem Statement</h2>
<p>Humans can solve many activities that are presented to us in our daily life without much effort, e.g. move and throw objects, open doors or write. However, although these tasks may seem simple, they require a certain degree of dexterous, which we humans learn through experience.</p>
<p><img src="images/dexterous_manipulation.png" style="display: block; margin: auto;" /></p>
</section><section id="current-approach" class="slide level2">
<h2>Current approach</h2>
<p>Trying to solve <em>these activities</em> with a robot following traditional robot control approaches represents a big challenge, because it is necessary to solve complex dynamic models and consider many uncertainties in the process.</p>
<p><img src="images/model_based_task.png" style="display: block; margin: auto;" /></p>
</section><section id="drl-approach" class="slide level2">
<h2>DRL approach</h2>
<p>However, the latest advances in DRL applied to robotic control have shown that it is possible to teach robots to solve this kind of tasks by trial and error.</p>
<p><img src="images/drl_diagram.png" style="display: block; margin: auto;" /></p>
</section><section id="where-to-get-the-data" class="slide level2">
<h2>Where to get the data?</h2>
<p>We can gather all the data necessary for the training in simulation, and then deploy the learned control policy in a physical robot.</p>
<video data-autoplay src="videos/fetch_push.mp4" loop="loop" width="300" height="300">
</video>
<video data-autoplay src="videos/shadow_hand_block.mp4" loop="loop" width="300" height="300">
</video>
<video data-autoplay src="videos/humanoid.mp4" loop="loop" width="300" height="300">
</video>
</section><section id="reality-gap" class="slide level2">
<h2>Reality Gap</h2>
<p>However, simulation environments do <strong>not represent</strong> the full complexity of the real world, and the policies learned in these virtual environments only perform well under conditions similar to those seen during the training phase. This disparity between virtual simulation environments and the real world is known as the <strong>reality gap</strong>.</p>
<p><img src="images/sim_to_real.png" style="display: block; margin: auto;" /></p>
</section><section id="dradr" class="slide level2">
<h2>DR/ADR</h2>
<p><span class="citation" data-cites="peng2018sim akkaya2019solving">(Peng et al. 2018; Akkaya et al. 2019)</span> shown that it is possible to overcome this barrier using techniques that <strong>randomly modify the dynamics of the simulator</strong> during the training phase in order to expose the agent to a wide range of variations in the environment, this forces the agent to <strong>learn to adapt</strong> to the constant changes in the environment.</p>
<p><img src="images/dr_crop.png" style="display: block; margin: auto;" /></p>
</section><section id="hypothesis" class="slide level2">
<h2>Hypothesis</h2>
<blockquote>
<p>If we randomly modify the dynamics of a simulation environment in order to learn a robust control policy for solving a dexterous manipulation task, then it is possible to transfer this learned policy to a physical robot and solve the task with little or any fine-tuning.</p>
</blockquote>
</section><section id="objectives" class="slide level2">
<h2>General objective</h2>
<p>The general objective of this thesis project is:</p>
<ul>
<li>Train an agent in simulation to solve a specific dexterous manipulation task using reinforcement learning algorithms with domain randomization techniques and then transfer the learned policy to a physical robot.</li>
</ul>
</section><section id="specific-objectives" class="slide level2">
<h2>Specific objectives</h2>
<p>The following objectives emerge from the main objective of this research:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Start</th>
<th style="text-align: right;">Duration</th>
<th style="text-align: left;">Type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Review Deep RL literature</td>
<td style="text-align: left;">2020-07-15</td>
<td style="text-align: right;">60</td>
<td style="text-align: left;">Research</td>
</tr>
<tr class="even">
<td style="text-align: left;">Select state-of-the-art simulator</td>
<td style="text-align: left;">2020-09-15</td>
<td style="text-align: right;">15</td>
<td style="text-align: left;">Research</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Learn to use the simulator</td>
<td style="text-align: left;">2020-09-30</td>
<td style="text-align: right;">60</td>
<td style="text-align: left;">Learning</td>
</tr>
<tr class="even">
<td style="text-align: left;">Create simulation environment</td>
<td style="text-align: left;">2020-12-01</td>
<td style="text-align: right;">30</td>
<td style="text-align: left;">Develop</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Train first agent without DR/ADR</td>
<td style="text-align: left;">2021-01-01</td>
<td style="text-align: right;">30</td>
<td style="text-align: left;">Develop</td>
</tr>
<tr class="even">
<td style="text-align: left;">Review state-of-the-art DR/ADR</td>
<td style="text-align: left;">2021-02-01</td>
<td style="text-align: right;">15</td>
<td style="text-align: left;">Research</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Tranin second agent with DR/ADR</td>
<td style="text-align: left;">2021-02-15</td>
<td style="text-align: right;">30</td>
<td style="text-align: left;">Develop</td>
</tr>
<tr class="even">
<td style="text-align: left;">Transfer policies to real robot</td>
<td style="text-align: left;">2021-03-15</td>
<td style="text-align: right;">30</td>
<td style="text-align: left;">Develop</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Make a comparative between both policies</td>
<td style="text-align: left;">2021-04-15</td>
<td style="text-align: right;">30</td>
<td style="text-align: left;">Analysis</td>
</tr>
<tr class="even">
<td style="text-align: left;">Write the thesis</td>
<td style="text-align: left;">2021-03-15</td>
<td style="text-align: right;">120</td>
<td style="text-align: left;">Writing</td>
</tr>
</tbody>
</table>
</section><section id="scientific-contribution" class="slide level2">
<h2>Scientific contribution</h2>
<p>The main contribution of this work is to solve a dexterous manipulation task (<em>probably throwing objects, like balls, out of the robot’s reach range</em>) with a redundant serial robot by training an agent in simulation.</p>
</section></section>
<section><section id="progress" class="title-slide slide level1"><h1>Current progress</h1></section><section id="section-2" class="slide level2">
<h2></h2>
<p>I’m working on the first specific objective “Review Deep RL literature”, for that, I’m actively reading the state-of-the-art papers and also the following RL/DRL books:</p>
<p><img src="images/rl_books_covers.png" width="100%" style="display: block; margin: auto;" /></p>
</section><section id="section-3" class="slide level2">
<h2></h2>
<p>In addition to the literature review work, I started taking two RL/ML courses:</p>
<ul>
<li>OpenAI’s <a href="https://spinningup.openai.com/en/latest/">Spinning Up in Deep RL</a></li>
</ul>
<p><img src="images/spinning-up-logo2.png" style="display: block; margin: auto;" /></p>
<ul>
<li>Google’s <a href="https://developers.google.com/machine-learning/crash-course">Machine Learning Crash Course</a></li>
</ul>
<p><img src="images/google_ml_course.png" width="50%" style="display: block; margin: auto;" /></p>
</section><section id="section-4" class="slide level2">
<h2></h2>
<p>Also, I have made progress on my second objective “Select state-of-the art simulation”. For my CAD course, I developed a project for path-planning in manufacturing process with robots on free-form surfaces, using one of the state-of-the-art robotics simulators (CoppeliaSim with PyRep ML toolkit).</p>
<center>
<video data-autoplay src="videos/pyrep_demo.m4v" loop="loop" width="800" height="450">
</video>
</center>
</section></section>
<section><section id="activities" class="title-slide slide level1"><h1>Chronogram of activities</h1></section><section id="section-5" class="slide level2">
<h2></h2>
<div id="htmlwidget-8d8963a3673a641b68cd" style="width:768px;height:576px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-8d8963a3673a641b68cd">{"x":{"visdat":{"3e1984adba384":["function () ","plotlyVisDat"]},"cur_data":"3e1984adba384","attrs":{"3e1984adba384":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2020-07-15","2020-09-13"],"y":[10,10],"mode":"lines","line":{"color":"#E78AC3","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Review Deep RL literature <br> Duration:  60 days<br> Type:  Research","evaluate":true,"inherit":true},"3e1984adba384.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2020-09-15","2020-09-30"],"y":[9,9],"mode":"lines","line":{"color":"#E78AC3","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Select state-of-the-art simulator <br> Duration:  15 days<br> Type:  Research","evaluate":true,"inherit":true},"3e1984adba384.2":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2020-09-30","2020-11-29"],"y":[8,8],"mode":"lines","line":{"color":"#8DA0CB","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Learn to use the simulator <br> Duration:  60 days<br> Type:  Learning","evaluate":true,"inherit":true},"3e1984adba384.3":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2020-12-01","2020-12-31"],"y":[7,7],"mode":"lines","line":{"color":"#FC8D62","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Create simulation environment <br> Duration:  30 days<br> Type:  Develop","evaluate":true,"inherit":true},"3e1984adba384.4":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2021-01-01","2021-01-31"],"y":[6,6],"mode":"lines","line":{"color":"#FC8D62","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Train first agent without DR/ADR <br> Duration:  30 days<br> Type:  Develop","evaluate":true,"inherit":true},"3e1984adba384.5":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2021-02-01","2021-02-16"],"y":[5,5],"mode":"lines","line":{"color":"#E78AC3","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Review state-of-the-art DR/ADR <br> Duration:  15 days<br> Type:  Research","evaluate":true,"inherit":true},"3e1984adba384.6":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2021-02-15","2021-03-17"],"y":[4,4],"mode":"lines","line":{"color":"#FC8D62","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Tranin second agent with DR/ADR <br> Duration:  30 days<br> Type:  Develop","evaluate":true,"inherit":true},"3e1984adba384.7":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2021-03-15","2021-04-14"],"y":[3,3],"mode":"lines","line":{"color":"#FC8D62","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Transfer policies to real robot <br> Duration:  30 days<br> Type:  Develop","evaluate":true,"inherit":true},"3e1984adba384.8":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2021-04-15","2021-05-15"],"y":[2,2],"mode":"lines","line":{"color":"#66C2A5","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Make a comparative between both policies <br> Duration:  30 days<br> Type:  Analysis","evaluate":true,"inherit":true},"3e1984adba384.9":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":["2021-03-15","2021-07-13"],"y":[1,1],"mode":"lines","line":{"color":"#A6D854","width":20},"showlegend":false,"hoverinfo":"text","text":"Task:  Write the thesis <br> Duration:  120 days<br> Type:  Writing","evaluate":true,"inherit":true}},"layout":{"width":900,"height":600,"margin":{"b":40,"l":60,"t":25,"r":10},"xaxis":{"domain":[0,1],"automargin":true,"showgrid":true,"tickfont":{"color":"#000000"},"title":[]},"yaxis":{"domain":[0,0.9],"automargin":true,"showgrid":true,"tickfont":{"color":"#000000"},"tickmode":"array","tickvals":[10,9,8,7,6,5,4,3,2,1],"ticktext":["Review Deep RL literature","Select state-of-the-art simulator","Learn to use the simulator","Create simulation environment","Train first agent without DR/ADR","Review state-of-the-art DR/ADR","Tranin second agent with DR/ADR","Transfer policies to real robot","Make a comparative between both policies","Write the thesis"],"title":[]},"legend":{"orientation":"h"},"hovermode":"closest","showlegend":false},"source":"A","config":{"showSendToCloud":false},"data":[{"x":["2020-07-15","2020-09-13"],"y":[10,10],"mode":"lines","line":{"color":"#E78AC3","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Review Deep RL literature <br> Duration:  60 days<br> Type:  Research","Task:  Review Deep RL literature <br> Duration:  60 days<br> Type:  Research"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2020-09-15","2020-09-30"],"y":[9,9],"mode":"lines","line":{"color":"#E78AC3","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Select state-of-the-art simulator <br> Duration:  15 days<br> Type:  Research","Task:  Select state-of-the-art simulator <br> Duration:  15 days<br> Type:  Research"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(255,127,14,1)","line":{"color":"rgba(255,127,14,1)"}},"error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2020-09-30","2020-11-29"],"y":[8,8],"mode":"lines","line":{"color":"#8DA0CB","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Learn to use the simulator <br> Duration:  60 days<br> Type:  Learning","Task:  Learn to use the simulator <br> Duration:  60 days<br> Type:  Learning"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(44,160,44,1)","line":{"color":"rgba(44,160,44,1)"}},"error_y":{"color":"rgba(44,160,44,1)"},"error_x":{"color":"rgba(44,160,44,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2020-12-01","2020-12-31"],"y":[7,7],"mode":"lines","line":{"color":"#FC8D62","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Create simulation environment <br> Duration:  30 days<br> Type:  Develop","Task:  Create simulation environment <br> Duration:  30 days<br> Type:  Develop"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(214,39,40,1)","line":{"color":"rgba(214,39,40,1)"}},"error_y":{"color":"rgba(214,39,40,1)"},"error_x":{"color":"rgba(214,39,40,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2021-01-01","2021-01-31"],"y":[6,6],"mode":"lines","line":{"color":"#FC8D62","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Train first agent without DR/ADR <br> Duration:  30 days<br> Type:  Develop","Task:  Train first agent without DR/ADR <br> Duration:  30 days<br> Type:  Develop"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(148,103,189,1)","line":{"color":"rgba(148,103,189,1)"}},"error_y":{"color":"rgba(148,103,189,1)"},"error_x":{"color":"rgba(148,103,189,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2021-02-01","2021-02-16"],"y":[5,5],"mode":"lines","line":{"color":"#E78AC3","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Review state-of-the-art DR/ADR <br> Duration:  15 days<br> Type:  Research","Task:  Review state-of-the-art DR/ADR <br> Duration:  15 days<br> Type:  Research"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(140,86,75,1)","line":{"color":"rgba(140,86,75,1)"}},"error_y":{"color":"rgba(140,86,75,1)"},"error_x":{"color":"rgba(140,86,75,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2021-02-15","2021-03-17"],"y":[4,4],"mode":"lines","line":{"color":"#FC8D62","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Tranin second agent with DR/ADR <br> Duration:  30 days<br> Type:  Develop","Task:  Tranin second agent with DR/ADR <br> Duration:  30 days<br> Type:  Develop"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(227,119,194,1)","line":{"color":"rgba(227,119,194,1)"}},"error_y":{"color":"rgba(227,119,194,1)"},"error_x":{"color":"rgba(227,119,194,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2021-03-15","2021-04-14"],"y":[3,3],"mode":"lines","line":{"color":"#FC8D62","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Transfer policies to real robot <br> Duration:  30 days<br> Type:  Develop","Task:  Transfer policies to real robot <br> Duration:  30 days<br> Type:  Develop"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(127,127,127,1)","line":{"color":"rgba(127,127,127,1)"}},"error_y":{"color":"rgba(127,127,127,1)"},"error_x":{"color":"rgba(127,127,127,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2021-04-15","2021-05-15"],"y":[2,2],"mode":"lines","line":{"color":"#66C2A5","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Make a comparative between both policies <br> Duration:  30 days<br> Type:  Analysis","Task:  Make a comparative between both policies <br> Duration:  30 days<br> Type:  Analysis"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(188,189,34,1)","line":{"color":"rgba(188,189,34,1)"}},"error_y":{"color":"rgba(188,189,34,1)"},"error_x":{"color":"rgba(188,189,34,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":["2021-03-15","2021-07-13"],"y":[1,1],"mode":"lines","line":{"color":"#A6D854","width":20},"showlegend":false,"hoverinfo":["text","text"],"text":["Task:  Write the thesis <br> Duration:  120 days<br> Type:  Writing","Task:  Write the thesis <br> Duration:  120 days<br> Type:  Writing"],"evaluate":true,"type":"scatter","marker":{"color":"rgba(23,190,207,1)","line":{"color":"rgba(23,190,207,1)"}},"error_y":{"color":"rgba(23,190,207,1)"},"error_x":{"color":"rgba(23,190,207,1)"},"xaxis":"x","yaxis":"y","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</section></section>
<section><section id="references" class="title-slide slide level1"><h1>References</h1></section><section id="section-6" class="slide level2">
<h2></h2>
<div id="refs" style="width: 1000px; height: 600px; overflow-y: scroll;">
<div id="ref-akkaya2019solving">
<p>Akkaya, Ilge, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, et al. 2019. “Solving Rubik’s Cube with a Robot Hand.” <em>arXiv Preprint arXiv:1910.07113</em>.</p>
</div>
<div id="ref-gao2020robotic">
<p>Gao, Wenbo, Laura Graesser, Krzysztof Choromanski, Xingyou Song, Nevena Lazic, Pannag Sanketi, Vikas Sindhwani, and Navdeep Jaitly. 2020. “Robotic Table Tennis with Model-Free Reinforcement Learning.” <em>arXiv Preprint arXiv:2003.14398</em>.</p>
</div>
<div id="ref-gu2017deep">
<p>Gu, Shixiang, Ethan Holly, Timothy Lillicrap, and Sergey Levine. 2017. “Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates.” In <em>2017 Ieee International Conference on Robotics and Automation (Icra)</em>, 3389–96. IEEE.</p>
</div>
<div id="ref-mnih2013playing">
<p>Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. “Playing Atari with Deep Reinforcement Learning.” <em>arXiv Preprint arXiv:1312.5602</em>.</p>
</div>
<div id="ref-OpenAI_dota">
<p>OpenAI. 2018. “OpenAI Five.” <a href="https://blog.openai.com/openai-five/">https://blog.openai.com/openai-five/</a>.</p>
</div>
<div id="ref-openai2018learning">
<p>OpenAI, Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, et al. 2018. “Learning Dexterous in-Hand Manipulation.” <a href="http://arxiv.org/abs/1808.00177">http://arxiv.org/abs/1808.00177</a>.</p>
</div>
<div id="ref-peng2018sim">
<p>Peng, Xue Bin, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. 2018. “Sim-to-Real Transfer of Robotic Control with Dynamics Randomization.” In <em>2018 Ieee International Conference on Robotics and Automation (Icra)</em>, 1–8. IEEE.</p>
</div>
<div id="ref-rajeswaran2017learning">
<p>Rajeswaran, Aravind, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. 2017. “Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations.” <em>arXiv Preprint arXiv:1709.10087</em>.</p>
</div>
<div id="ref-silver2016mastering">
<p>Silver, David, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, et al. 2016. “Mastering the Game of Go with Deep Neural Networks and Tree Search.” <em>Nature</em> 529 (7587): 484–89.</p>
</div>
<div id="ref-silver2017mastering">
<p>Silver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.” <em>arXiv Preprint arXiv:1712.01815</em>.</p>
</div>
<div id="ref-song2020rapidly">
<p>Song, Xingyou, Yuxiang Yang, Krzysztof Choromanski, Ken Caluwaerts, Wenbo Gao, Chelsea Finn, and Jie Tan. 2020. “Rapidly Adaptable Legged Robots via Evolutionary Meta-Learning.” <em>arXiv Preprint arXiv:2003.01239</em>.</p>
</div>
<div id="ref-tan2018sim">
<p>Tan, Jie, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke. 2018. “Sim-to-Real: Learning Agile Locomotion for Quadruped Robots.” <em>arXiv Preprint arXiv:1804.10332</em>.</p>
</div>
<div id="ref-tobin2017domain">
<p>Tobin, Josh, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. 2017. “Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World.” In <em>2017 Ieee/Rsj International Conference on Intelligent Robots and Systems (Iros)</em>, 23–30. IEEE.</p>
</div>
<div id="ref-zeng2020tossingbot">
<p>Zeng, Andy, Shuran Song, Johnny Lee, Alberto Rodriguez, and Thomas Funkhouser. 2020. “Tossingbot: Learning to Throw Arbitrary Objects with Residual Physics.” <em>IEEE Transactions on Robotics</em>.</p>
</div>
</div>
</section><section id="section-7" class="slide level2">
<h2></h2>
<br>
<center>
<h1>
Thank you for your attention ☕
</h1>
</center>
</section></section>
    </div>
  </div>

  <script src="slides_files/reveal.js-3.3.0.1/lib/js/head.min.js"></script>
  <script src="slides_files/reveal.js-3.3.0.1/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display the page number of the current slide
        slideNumber: true,
        // Push each slide change to the browser history
        history: true,
        // Vertical centering of slides
        center: true,
        // Opens links in an iframe preview overlay
        previewLinks: false,
        // Transition style
        transition: 'none', // none/fade/slide/convex/concave/zoom
        // Transition style for full page slide backgrounds
        backgroundTransition: 'default', // none/fade/slide/convex/concave/zoom



        // Optional reveal.js plugins
        dependencies: [
        ]
      });
    </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

<script>
  (function() {
    if (window.jQuery) {
      Reveal.addEventListener( 'slidechanged', function(event) {  
        window.jQuery(event.previousSlide).trigger('hidden');
        window.jQuery(event.currentSlide).trigger('shown');
      });
    }
  })();
</script>


  </body>
</html>
