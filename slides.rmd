---
title: "Learning Dexterous Manipulation with Model-Free Deep Reinforcement Learning"
author: |
  | <small>Thesis Director: Ph.D. Reyes Ríos Cabrera</small>
  | <small>Author: Luis Castillo</small>
output:
  revealjs::revealjs_presentation:
    self_contained: false
    theme: white
    highlight: haddock
    transition: none
    center: true
    fig_caption: false
    css: style.css
    reveal_options:
      slideNumber: true
      previewLinks: false
bibliography: references.bib
nocite: '@*'
---
## Presentation outline

* [Introduction](#introduction)
* [State-of-the-art](#state-of-the-art)
* [Proposal Description](#proposal-description)
  + [Motivation](#motivation)
  + [Background](#background)
  + [Problem Statement](#problem-statement)
  + [Hypothesis](#hypothesis)
  + [Objectives](#objectives)
  + [Scientific Contribution](#scientific-contribution)
* [Current Progress](#progress)
* [Chronogram of activities](#activities)
* [References](#references)

# Introduction {#introduction}

##
**Deep Reinforcement Learning** have great potential in real-world applications. Recent advances have shown that these methods can solve tasks with superior performance than humans in some scenarios, e.g. classic Atari games @mnih2013playing, complicated board games [@silver2016mastering; @silver2017mastering] or massive online multiplayer games @OpenAI_dota.

<video data-autoplay src="videos/alphazero_2017_final.m4v" loop="loop" width="460" height="258"></video>
<video data-autoplay src="videos/openai_five_dota_final.m4v" loop="loop" width="460" height="258"></video>

##
DRL is not just for games. Due to the simplicity of the RL framework. The same algorithms for solving games can be used to control physical robots in real-time @gao2020robotic. However,  DRL methods **are not intended to completely replace ** traditional control approaches. But, these methods can be applied in **specific scenarios** or in combination with traditional methods.

<video data-autoplay src="videos/dxl-tracker-trpo.m4v" loop="loop" width="280" height="157"></video>
<video data-autoplay src="videos/create-docker-trpo.m4v" loop="loop" width="280" height="157"></video>
<video data-autoplay src="videos/ur-reacher-6-trpo.m4v" loop="loop" width="280" height="157"></video>

# State of the art {#state-of-the-art}

## RL algorithms
```{r rl_algos, echo=FALSE, fig.align='center'}
knitr::include_graphics("images/rl_algorithms.svg")
```

## Sim-To-Real (1/4)
@peng2018sim propose to use dynamics randomization (DR) to train in simulation, and deploy the learned policies directly on a physical robot to solve pushing object task, achieving good performance.

<center>
  <video data-autoplay src="videos/peng2018.m4v" loop="loop" width="720" height="405"></video>
</center>

## Sim-To-Real (2/4)
@openai2018learning propose to learn dexterity of in-hand manipulation to perform object reorientation for a five-finger robotic hand, using PPO with DR in simulation. They use the same code for playing Dota 2 @OpenAI_dota.

<center>
  <video data-autoplay src="videos/shadow_hand_real_block_final.m4v" loop="loop" width="720" height="405"></video>
</center>

## Sim-To-Real (3/4)
@tan2018sim propose to learn locomotion for quadruped robots. They learn control policies in simulation and deployed on a quadruped robot. They perform system identification for tuning the simulation parameters to improve the fidelity of the physics simulator.

<center>
  <video data-autoplay src="videos/tan2018.m4v" loop="loop" width="520" height="292"></video>
</center>

## Sim-To-Real (4/4)
@zeng2020tossingbot propose an end-to-end AI system for picking and throwing objects. Achieving 85% of accuracy, more than double that achieved with analytical solutions (40%). They train the policy in simulation and then use an automatic system for fine-tuning on physical robot.

<center>
  <video data-autoplay src="videos/tossingbot.m4v" loop="loop" width="520" height="292"></video>
</center>

# Proposal description {#proposal-description}

## Motivation {#motivation}
In most cases, the dynamic process of **robotic control** can be approximated as an **MDP** making it an ideal field to experiment with DRL algorithms.

In addition, recently, large technology companies and prestigious research centers have focused their research in this area, such as OpenAI, who solved the Rubik's cube with a five-finger articulated robotic hand completely in simulation.

<video data-autoplay src="videos/shadow_rubik_normal.m4v" loop="loop" width="460" height="258"></video>
<video data-autoplay src="videos/shadow_rubik_oclution.m4v" loop="loop" width="460" height="258"></video>

## Training in a physical world
We can use DRL algorithms to train robust control policies directly in the physical robot. But this approach has some disadvantages:

* **Sampling inefficient**: The training may take thousands of hours to solve a single task.

* **Safety issues**: DRL methods use exploration techniques that can be result in dangerous actions.

## Training in simulation
Training in simulation and then transferring learned policy to physical systems (i.e, **sim-to-real**) or using expert human demonstrations are two approaches that satisfy computational and safety requirements in robot learning tasks.

<video data-autoplay src="videos/shadow_hand_block.mp4" loop="loop" width="292" height="292"></video>
<video data-autoplay src="videos/shadow_hand_real_block_final.m4v" loop="loop" width="520" height="292"></video>

## Background: RL {#background}
Reinforcement Learning (RL) is a subfield of ML that address the problem of automatic learning for optimal decisions over time. The main characters of the RL are the agent and the environment. The environment is the world where the agent lives and with which he interacts.

```{r rl_loop, echo=FALSE, fig.align='center', out.width='70%'}
knitr::include_graphics("images/agent_env_loop3.png")
```

## Deep Reinforcement Learning
DRL combine the advantages of DL and RL for building AI systems. The main reason to use DL in RL is to leverage the scalability of DNN in high-dimensional space. @mnih2013playing prove this by training a DNN from raw pixels to play classic arcade games.

<center>
  <video data-autoplay src="videos/rl_atari_final.m4v" loop="loop" width="520" height="292"></video>
</center>


## Problem Statement {#problem-statement}
Humans can solve many activities that are presented to us in our daily life without much effort, e.g. move and throw objects, open doors or write. However, although these tasks may seem simple, they require a certain degree of dexterous, which we humans learn through experience.

```{r tasks, echo=FALSE, fig.align='center'}
knitr::include_graphics("images/dexterous_manipulation.png")
```

## Current approach
Trying to solve *these activities* with a robot following traditional robot control approaches represents a big challenge, because it is necessary to solve complex dynamic models and consider many uncertainties in the process.

```{r model_diagram, echo=FALSE, fig.align='center'}
knitr::include_graphics("images/model_based_task.png")
```

## DRL approach
However, the latest advances in DRL applied to robotic control have shown that it is possible to teach robots to solve this kind of tasks by trial and error.

```{r drl_diagram, echo=FALSE, fig.align='center'}
knitr::include_graphics("images/drl_diagram.png")
```

## Where to get the data?
We can gather all the data necessary for the training in simulation, and then deploy the learned control policy in a physical robot.

<video data-autoplay src="videos/fetch_push.mp4" loop="loop" width="300" height="300"></video>
<video data-autoplay src="videos/shadow_hand_block.mp4" loop="loop" width="300" height="300"></video>
<video data-autoplay src="videos/humanoid.mp4" loop="loop" width="300" height="300"></video>

## Reality Gap
However, simulation environments do **not represent** the full complexity of the real world, and the policies learned in these virtual environments only perform well under conditions similar to those seen during the training phase. This disparity between virtual simulation environments and the real world is known as the **reality gap**.

```{r sim_to_real, echo=FALSE, fig.align='center'}
knitr::include_graphics("images/sim_to_real.png")
```


## DR/ADR
[@peng2018sim; @akkaya2019solving] shown that it is possible to overcome this barrier using techniques that **randomly modify the dynamics of the simulator** during the training phase in order to expose the agent to a wide range of variations in the environment, this forces the agent to **learn to adapt** to the constant changes in the environment.

```{r dr_hand, echo=FALSE, fig.align='center'}
knitr::include_graphics("images/dr_crop.png")
```

## Hypothesis {#hypothesis}

> If we randomly modify the dynamics of a simulation environment in order to learn a robust control policy for solving a dexterous manipulation task, then it is possible to transfer this learned policy to a physical robot and solve the task with little or any fine-tuning.

## General objective {#objectives}
The general objective of this thesis project is:

* Train an agent in simulation to solve a specific dexterous manipulation task using reinforcement learning algorithms with domain randomization techniques and then transfer the learned policy to a physical robot.

## Specific objectives
The following objectives emerge from the main objective of this research:

```{r objetives, echo=FALSE}
# Read in data
df <- read.csv("thesis-activities.csv", stringsAsFactors = FALSE)
# Convert to dates
df$Start <- as.Date(df$Start, format = "%d/%m/%Y")
# Create table
knitr::kable(df, booktabs = TRUE)
```

## Scientific contribution {#scientific-contribution}
The main contribution of this work is to solve a dexterous manipulation task (*probably throwing objects, like balls, out of the robot's reach range*) with a redundant serial robot by training an agent in simulation.

# Current progress {#progress}

##
I’m working on the first specific  objective “Review Deep RL literature”, for that, I’m actively reading the state-of-the-art papers and also the following RL/DRL books:

```{r books, echo=FALSE, fig.align='center', out.width='100%'}
knitr::include_graphics("images/rl_books_covers.png")
```

##
In addition to the literature review work, I started taking two RL/ML courses:

* OpenAI's [Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/)

```{r openai_course_logo, echo=FALSE, fig.align='center'}
knitr::include_graphics("images/spinning-up-logo2.png")
```

* Google's [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course)

```{r google_course_logo, echo=FALSE, fig.align='center', out.width='50%'}
knitr::include_graphics("images/google_ml_course.png")
```

##
Also, I have made progress on my second objective "Select state-of-the art simulation". For my CAD course, I developed a project for path-planning in manufacturing process with robots on free-form surfaces, using one of the state-of-the-art robotics simulators (CoppeliaSim with PyRep ML toolkit).

<center>
  <video data-autoplay src="videos/pyrep_demo.m4v" loop="loop" width="800" height="450"></video>
</center>

# Chronogram of activities {#activities}

##
```{r activities_chart, echo=FALSE, message=FALSE, warning=FALSE}
library(plotly)

# # Read in data
# df <- read.csv("thesis-activities.csv", stringsAsFactors = FALSE)
# # Convert to dates
# df$Start <- as.Date(df$Start, format = "%d/%m/%Y")

# Choose colors based on number of resources
cols <- RColorBrewer::brewer.pal(length(unique(df$Type)), name = "Set2")
df$color <- factor(df$Type, labels = cols)

# Initialize empty plot
fig <- plot_ly()

# Each task is a separate trace
# Each trace is essentially a thick line plot
# x-axis ticks are dates and handled automatically
for(i in 1:nrow(df)){
 fig <- add_trace(fig,
                 x = c(df$Start[i], df$Start[i] + df$Duration[i]),  # x0, x1
                 y = c(nrow(df)-i+1, nrow(df)-i+1),  # y0, y1
                 mode = "lines",
                 line = list(color = df$color[i], width = 20),
                 showlegend = FALSE,
                 hoverinfo = "text",
                 # Create custom hover text
                 text = paste("Task: ", df$Task[i], "<br>",
                              "Duration: ", df$Duration[i], "days<br>",
                              "Type: ", df$Type[i]),
                  evaluate = TRUE  # needed to avoid lazy loading
  )
}

# Add information to plot and make the chart more presentable
fig <- layout(fig,
            # Axis options:
            # 1. Remove gridlines
            # 2. Customize y-axis tick labels and show task names instead of numbers
            xaxis = list(showgrid = TRUE, tickfont = list(color = "#000000")),
            yaxis = list(showgrid = TRUE, tickfont = list(color = "#000000"),
                         tickmode = "array", tickvals = nrow(df):1, ticktext = unique(df$Task),
                         domain = c(0, 0.9)),
            width=900, height=600,
            legend=list(orientation='h')
            )
fig
```

# References {#references}

##
<div id="refs" style="width: 1000px; height: 600px; overflow-y: scroll;"></div>

##
<br>
<center>
  <h1>Thank you for your attention ☕</h1>
</center>